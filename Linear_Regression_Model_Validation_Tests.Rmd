---
title: "EMEA DCM MRM Independent Testing"
author: "Suraj"
date: "4 September 2018"
output:
  html_document: default
---

```{r, message= FALSE,warning=FALSE}
## Reading the Packages ####
loadlibraries = function(){
library('readxl')
library('Hmisc')
library('ggplot2')
library('plyr')
library('dplyr')
library('tidyr')
library('stringr')
library('ggplot2')
library('olsrr')
library('car')
library('stats')
library('urca')
library('tseries')
library('lmtest')
library('psych')
library('ppcor')
library('mctest')
library('scales')
library('zoo')
library('strucchange')
library('gsubfn')
library('writexl')
library('Emcdf')
library('gridExtra')
library('plot3D')
library('boot')
library('leaps')
#library('foreach')
#library('combinat')
library('doParallel')
#library('Rcpp')
library('installr')
# library('rgl') 
print('All Libraries Loaded')
}

loadlibraries()
# 
# tempdir()
# 
# 
# install.ed
# install.URL( exe_URL = http://justgetflux.com/dlwin.html, keep_install_file = TRUE, down)
# install.URL('http:\\update.justgetflux.com\\flux-setup.exe', download_dir = 'M:\\')

choose(210,3)
choose(30,3)



```


# Some Functions

```{r, message=FALSE}

## Custom Functions ####

# QoQ Function 
growth_transform = function(new, old){
  return((new-old)/old)
}

## More QoQ Functions

#QoQ Percent
qoq_p = function(var){
  n = length(var)
  v = rep(NA,n)
  for(i in 2:n){
    v[i] = (var[i]-var[i-1])/var[i-1]
  }
  return(v)
}

#QoQ Difference
qoq = function(var){
  n = length(var)
  v = rep(NA,n)
  for(i in 2:n){
    v[i] = var[i] - var[i-1]
  }
  return(v)
}

#L1 functions
L1 = function(var){
  n = length(var)
  v = rep(NA,n)
  for(i in 2:n){
    v[i] = var[i-1]
  }
  return(v)
}




# Function to plot graphs iteratively
draw_time_series_plot = function(df, y_label,
                                 x_label = 'Time',legend = 'none',text_angle =45){
  df$time = as.yearqtr(df$time, format = "%Y Q%q")
  qtr_breaks = seq(from = min(df$time), to = max(df$time), by = 0.25) # Puting label for each qtr
  
  
  # Transforming Data Frame to Make Plots
  long_df = df %>% gather(key = 'type',value = value,-time)
  
  #long_df
  color_palette <- c("red","blue","black",'purple','brown')
  
  
  
  pl = ggplot(long_df, aes(x = time, y = value)) + 
                  geom_line(aes(color = type), size = 1) + 
                  geom_point(aes(color = type, shape = type), size = 2) + 
                  scale_color_manual(values = color_palette) + theme_bw()
  
  pl2 = pl  + scale_x_yearqtr(breaks = qtr_breaks,
                          format = '%Y Q%q')+
                          theme(axis.text.x=element_text(angle=text_angle, vjust=0.5))
  pl3 = pl2 + ylab(y_label)+ xlab(x_label) + 
              theme(panel.grid.major = element_line(size = 0.2, linetype = 'dashed',
                      colour = "black") , legend.position = legend)
  
  return(pl3)
}


# RMSE Calculation
  RMSE = function(p,a){
    return( sqrt( mean( (p-a)^2,na.rm = TRUE  ) ) )
  }

# MAE Calculation
  MAE = function(p,a){
    return( mean( abs(p-a),na.rm = TRUE ) )
  }
  
# MAR Calculation # Slght difference from RAT
  MAR = function(a){
    n = length(a)
    s = 0
    for(i in 2:n){
      s = s+ abs(a[i]-a[i-1])
    }
    return(s/(n-1))
  }

  
# # RMSE/STD Function
#   RMSE_SD = function(p,a){
#     rmse = RMSE(p,a)
#     sd = sd(a,na.rm = TRUE)  # slight different from RAT
#     return(rmse/sd)
#   }
# 
# # MAE/MAR Function
#   MAE_MAR = function(p,a){
#     return(MAE(p,a)/MAR(a))
#   }

# CERRPCT
  CERRPCT = function(p,a){
    return(sum(p,na.rm = TRUE)/sum(a,na.rm = TRUE)  -1  )
  }


  

# Performance Metric calcualiton
  
  performance_metric = function(df, filter, is_mar_and_sd_out_of_filter = FALSE,
                                select_vars = c('street_fees','predictions')){
    
    df= subset(df, select = select_vars)
    df = cbind(df,filter)
    #df = df %>% drop_na()
    
    filter_df = df %>% filter(filter == TRUE)
    non_filter_df = df %>% filter(filter == FALSE)
    
    p = filter_df[, select_vars[2]]
    a = filter_df[, select_vars[1]]
    
    pm_df = as.data.frame(rep(NA,7),row.names = c('RMSE','MAE','MAR','SD','CERRPCT','RMSE/SD','MAE/MAR'))
    colnames(pm_df) = c('Values')
    
    pm_df[1,]= RMSE(p,a)
    pm_df[2,] = MAE(p,a)
    pm_df[5,] = CERRPCT(p,a)
    
    if(is_mar_and_sd_out_of_filter == TRUE){
      a = non_filter_df[,select_vars[1]]
        } 
    pm_df[3,] = MAR(a)
    pm_df[4,] = sd(a,na.rm = TRUE)
    pm_df[6,] = pm_df[1,]/pm_df[4,]
    pm_df[7,] = pm_df[2,]/pm_df[3,]
    return(pm_df)
  }
  

  # Taken From https://adairama.wordpress.com/2017/11/22/how-to-merge-multiple-datasets-in-r-based-on-row-names/
  multimerge <- function (mylist) {
  ## mimics a recursive merge or full outer join
 
  unames <- unique(unlist(lapply(mylist, rownames)))
 
  n <- length(unames)
 
  out <- lapply(mylist, function(df) {
 
    tmp <- matrix(nr = n, nc = ncol(df), dimnames = list(unames,colnames(df)))
    tmp[rownames(df), ] <- as.matrix(df)
    rm(df); gc()
 
    return(tmp)
  })
 
  stopifnot( all( sapply(out, function(x) identical(rownames(x), unames)) ) )
 
  bigout <- do.call(cbind, out)
  colnames(bigout) <- paste(rep(names(mylist), sapply(mylist, ncol)), unlist(sapply(mylist, colnames)), sep = "_")
  return(bigout)
  }
  
  
  
  
# Taken from http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/

# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}


```

# Setting up Directory 
```{r}
# Importing the data ####

setwd('M:\\Model Validation\\IBCM EMEA DCM Model Validation\\R Code')

create_basic_data = function(input_file_path = 'emea_dcm_mrm_testing_data.csv'){
    mrm_df = as.data.frame(read.csv(input_file_path))
    head(mrm_df)
    summary(mrm_df)
    
    colnames(mrm_df)
    # Changing the Names
    colnames(mrm_df) = c('time', 'eur_gdp','iTraxx',
                         'CDX','D1','D2','street_fees','cs_fees')
    
    # Splitting the time column in Quarter and Year
    temp = str_split_fixed(mrm_df$time,' ',2)
    mrm_df$quarter = temp[,1]
    mrm_df$year = as.numeric(temp[,2])
    mrm_df$time = paste(mrm_df$year, mrm_df$quarter)
    
    # Creation of QoQ on CDX
    mrm_df$L1_CDX = Lag(mrm_df$CDX,shift = 1)
    mrm_df$QoQ_CDX = mapply(growth_transform, mrm_df$CDX, mrm_df$L1_CDX)
    
    
    # Creation of QoQ on iTraxx
    mrm_df$L1_iTraxx = Lag(mrm_df$iTraxx,shift = 1)
    mrm_df$QoQ_iTraxx = mapply(growth_transform, mrm_df$iTraxx, mrm_df$L1_iTraxx)
    
    
    # Creation of t(index variable)
    mrm_df$t = 1:dim(mrm_df)[1]
    
    # Creation of SoW variable
    mrm_df$SoW = mrm_df$cs_fees/mrm_df$street_fees
    
    # Creating yearqtr Variable
    mrm_df$yearqtr = as.yearqtr(mrm_df$time,format = "%Y Q%q")
    
    # Changing the row names to time variable
    row.names(mrm_df) = mrm_df$time
    
    head(mrm_df)
    return(mrm_df)
}

mrm_df = create_basic_data()

```


```{r}
# Save a mrm_df to a file
saveRDS(mrm_df, "mrm_df.rds")

# Restore it under a different name
#my_mrm_df = readRDS("mrm_df.rds")# Changing the Names
```

# 0. Behaviour of Time Plots
```{r}

variable_list = c('street_fees','CDX','QoQ_CDX','iTraxx','eur_gdp','D1','D2')

plot_time_trend = function(var_list,mrm_df){
    # Making a Plot
  
  df = mrm_df %>% drop_na()
  year_breaks = seq(from = min(df$yearqtr), to = max(df$yearqtr), by = 1)
  qtr_breaks = seq(from = min(df$yearqtr), to = max(df$yearqtr), by = 0.25) # Puting label for each qtr
  
  for(var in var_list){
    
    pl = ggplot(df, aes_string(x = 'yearqtr', y = var)) + 
            geom_line(size = 1,color = 'red') + geom_point(size = 1.5) + theme_bw()
    pl2 = pl  + scale_x_yearqtr(breaks = qtr_breaks,# breaks = year_breaks,
                            format = '%Y Q%q')+
                            theme(axis.text.x=element_text(angle=90, vjust=0.5))
    pl3 = pl2 + ylab(var)+ xlab('Time') + theme(panel.grid.major = element_line(size = 0.2, linetype = 'dashed',
                                colour = "black") )
    print(pl3)
  }
}

plot_time_trend(variable_list,mrm_df)

```


## 1. IMPLEMENTAITON CHECK ON THE lm_mrm ####

```{r}
lm_mrm = lm(street_fees ~   QoQ_CDX+ iTraxx + eur_gdp + D1 + D2 , data = mrm_df)
summary(lm_mrm)

```

# Trivial Assumptions
```{r}

# Street Fees drive cs fees

summary(lm(cs_fees ~ street_fees -1, data = mrm_df))

cor(mrm_df$cs_fees, mrm_df$street_fees)
draw_time_series_plot(subset(mrm_df,select = c('cs_fees','street_fees','time')),y_label = 'Fees',x_label = 'Time',legend = 'right')


# SoW
summary(lm(SoW ~ street_fees, data = mrm_df))
summary(lm(SoW ~ lag(street_fees,n = 2), data = mrm_df))

summary(lm(SoW ~ QoQ_CDX+ iTraxx + eur_gdp + D1 + D2, data = mrm_df))
summary(lm(SoW ~ QoQ_CDX, data = mrm_df))
summary(lm(SoW ~ iTraxx, data = mrm_df))
summary(lm(SoW ~ eur_gdp, data = mrm_df))
summary(lm(SoW ~ D1, data = mrm_df))
summary(lm(SoW ~ D2, data = mrm_df))
```

# Segmentation Analysis
```{r}
cutoff_t = as.yearqtr('2006 Q1')

corp_fig_df = as.data.frame(read_xlsx('corporate_fig.xlsx',sheet = 'Summary'))

corp_fig_df$time = as.yearqtr(corp_fig_df$Quarter,format = '%Y Q%q')
corp_fig_df$Quarter = NULL
str(corp_fig_df)

corp_fig_df = corp_fig_df[corp_fig_df$time >= cutoff_t,]
head(corp_fig_df)
tail(corp_fig_df)

mrm_df$street_fees_FIG = c(corp_fig_df$`DCM Street FIG`, NA)
mrm_df$street_fees_CORP = c(corp_fig_df$`DCM Street Corp`, NA)
tail(mrm_df)


corp_m = lm(street_fees_FIG ~   QoQ_CDX+ iTraxx + eur_gdp + D1 + D2 , data = mrm_df)
summary(corp_m)

summary(lm(street_fees_CORP ~   QoQ_CDX+ iTraxx + eur_gdp + D1 + D2 , data = mrm_df))



```


# OLS Assumption Testing


1. Linearity
  + Partial Residual Plot
  + Ramsey RESET
2. Normality
  + Q-Q Plot
  + Kolmogorov - Smirnov Test
  + Shapiro-wilk Test
  + Jarque-Bera Test
3. Heteroscedasticity
  + Residual vs Fitted
  + Breusch Pagan Test
  
4. Multicollinearity
    + VIF
    + Partial Correlation
5. Auto Correlation
  + ACF/PACF
  + Breusch-Godfrey 
  + Ljung-Box
6. Seasonality
  + Box Plot
  + Kruskal- Wallis Test





```{r}


# Test Start

## Creating Regression dataset to remove NA values
  keep_col = c('QoQ_CDX','iTraxx','eur_gdp','street_fees','D1','D2')
  regdata = subset(mrm_df[-1,],select = keep_col)
  regdata$residuals = lm_mrm$residuals
  regdata$fitted_values = lm_mrm$fitted.values
  head(regdata,5)
  tail(regdata,5)
```

##A. Linearity Checks


```{r}
  ## Linearity or lm_mrm Misspecification##
  
  # Residual Plot against all 3 MEFs and fitted values
  var_list = c('QoQ_CDX','iTraxx','eur_gdp','fitted_values')
  for(var in var_list){
    #file_name = paste('Figures/OLS Assumptions/Linearity/The Plot of Residuals vs ', var,'.png',sep ='')
    #Making the Plot
    pl = ggplot(regdata,aes_string(y = 'residuals',x = var))
    pl2 = pl+ geom_point()
    print(pl2)
    #ggsave(file_name, plot = last_plot(),dpi = 300)
  }

  
  # # Partial Regression Plots
  # mef_list = c('QoQ_CDX','iTraxx','eur_gdp')
  # mef = 'eur_gdp'
  # 
  # 
  # paste(mef_list,sep = 'e')
  # y_lm = lm(street_fees ~ QoQ_CDX + iTraxx, data = regdata)
  # mef_lm = lm(eur_gdp ~ QoQ_CDX + iTraxx, data = regdata)
  # y_res = y_lm$residuals
  # mef_res = mef_lm$residuals
  
  
  # Partial Residual Plot

  # Library Funtion
  library(car)
  crPlots(lm_mrm,ask = F)
  
  
  # My Code
  
  # Ramsey Reset Test
 
  summary(lm_mrm)
  # Power 2
  resettest(lm_mrm,type = 'fitted',power = 2)
  resettest(lm_mrm,type = 'fitted',power = c(2,4,5))
  resettest(lm_mrm,type = 'fitted')
  
  
  # Power 3
  resettest(lm_mrm,type = 'fitted',power = 3)
  
  # Power 3
  resettest(lm_mrm,type = 'fitted',power = 4)
  
```  


##B. Normality Checks

```{r}
## Normality
    # Normal Q-Q Plot
    qqnorm(regdata$residuals)
  
    
    # Histogram Plot
    ggplot(data = regdata,aes(x = residuals)) + geom_histogram(binwidth = 10)
    
    # KS test
    standaridzed_res = regdata$residuals/sqrt(var(regdata$residuals))
    ks.test(x =standaridzed_res,y =pnorm,alternative = c('two.sided')) # So Input need sto be standarized 
    
    
    # Shapiro - Wilk Test
    shapiro.test(regdata$residuals)
    #shapiro.test(standaridzed_res)
    
    # Jarque-Bera Test
    jarque.bera.test(regdata$residuals)
    #jarque.bera.test(standaridzed_res)
```  

##C. Heteroscedasticity
```{r}
## Heteroscedasticity
    
    
      # Residuals Vs Fitted Values PLots
      ggplot(regdata,aes(x = fitted_values,y = residuals))+ geom_point() + geom_smooth(method = 'lm') #+  stat_smooth_func(geom="text",method="lm",hjust=0,parse=TRUE)
      
      # BP Test
      bptest(lm_mrm)
      
```
   
##D. Multicollinearity      
```{r}      
## Multicollinearity
    # VIF Test
      vif(lm_mrm)
    # Pair Plots
      
      pairs(~QoQ_CDX + iTraxx + eur_gdp,data=regdata,lower.panel = NULL, 
            main="Simple Scatterplot Matrix")
      
      
      # Checking for correlations
      
      pairs.panels(regdata[,1:3], 
                   method = "pearson", # correlation method
                   hist.col = "#00AFBB",
                   density = TRUE,  # show density plots
                   ellipses = TRUE # show correlation ellipses
      )
      cor(regdata[,1:3])
      
      
      # Checking for partial correlation
      
      
      # # Given eur_Gdp
      # pcor.test(regdata$QoQ_CDX,regdata$iTraxx,regdata$eur_gdp)
      # 
      # #Given itraxx
      # pcor.test(regdata$eur_gdp,regdata$QoQ_CDX,regdata$iTraxx)
      # 
      # # Given QoQ_CDX
      # pcor.test(regdata$iTraxx,regdata$eur_gdp,regdata$QoQ_CDX)
      # 
      # pcor.test(regdata$eur_gdp,regdata$iTraxx,regdata$QoQ_CDX)
      # pcor.test(regdata$eur_gdp,regdata$iTraxx,regdata$QoQ_CDX)
      
      # Combine data
      pcor(regdata[,1:3])
      cor(regdata[,1:3])
      ggplot(data =regdata,aes(x = iTraxx,y = eur_gdp)) + geom_point() + geom_smooth(method = 'lm') 
      
      
      # Checking for assumption
      
      summary(lm(cs_fees ~ street_fees, data = mrm_df))
     
     
      
      
      
      
```
## Checkfor issue of Multicollinearity 


```{r}

  # Checking for the Threshold value
  r_iTraxx_eur_gdp = cor(regdata[,1:3])[3,2]
      r_iTraxx_eur_gdp
      1/(1- r_iTraxx_eur_gdp^2)  
      1/(1 - 0.67^2)
      # Checking for critical value
      
      VIF = c(1:20)
      
      viftocor = function(vif){
        return(sqrt(  1 - 1/vif  ))
      }
      
      sapply(X = VIF,FUN = viftocor)
      
      
      itraxx_lm = lm(iTraxx ~ QoQ_CDX + eur_gdp + D1 + D2, data = mrm_df)
      1/(1-summary(itraxx_lm)$r.squared)
       1/(1-0.36)
       
       
  
```
## Condition Index test for Multicollinearity

```{r}
#

  x = subset(mrm_df[-1,],select =c('QoQ_CDX','iTraxx','eur_gdp','D1','D2'))
  # x$mc_iTrxx = x$iTraxx + rnorm(length(x$iTraxx))
  # head(x)
  scaled_x = scale(x)
  
  X_Prime_X = t(x) %*% as.matrix(x)
  X_Prime_X
  eigen_values = eigen(cor(x))$values
  eigen_values_2 = eigen(X_Prime_X)$values
  
  condition_index = function(e_v){
    return(sqrt(  max(e_v)/min(e_v) ))
  }
  
  condition_index(eigen_values)
  condition_index(eigen_values_2)
  
  # kappa(X_Prime_X)
  # scale(x)
  # 
  # length(x$iTraxx)
  # colMeans(x)
  # apply(x,2,sd)
  # y = d[,1]
  # x = d[,-1]
  # 
  # omcdiag(x,y)
  # imcdiag(x,y,all = T)  
  # mc.plot(x,y)
  # 
  # sqrt(6.5503)
  # 
  # mean_itrax = mean(x$iTraxx)
  # sd_itrax = sd(x$iTraxx)
  # 
  # head(scaled_x,1)
  # 
  # (x$iTraxx[1] -mean_itrax)/sd_itrax
  
```

##E. Autocorrelation

```{r}
      
## Autocorrelation
  
  
  # Plot of residuals vs time
  regdata$t = 2:47
  pl = ggplot(regdata,aes(y = residuals,x = t))
  pl2 = pl+ geom_point()+ geom_smooth(method ='lm',show.legend = T)
  pl2
  
  
  # BreaushcGodfreey Test
  for(i in 1:8){
    print(bgtest(lm_mrm,order = i, type = 'Chisq'))
  }
  # Ljung-Box Test
  for(i in 1:4){
    print(Box.test(lm_mrm$residuals,lag = i,type = 'Ljung',fitdf = 0))
  }
  
  # Durbin-Watson Test(DW Test)
  dwtest(lm_mrm,alternative = 'less')
  dwtest(lm_mrm,alternative = 'greater')
  dwtest(lm_mrm, alternative = 'two.sided') # to discus with arghya on it
  d_L = 1.238
  d_U = 1.835
  4-d_L
  4-d_U
  
  # ACF & PACF plots
  acf(lm_mrm$residuals, main = 'ACF of Residuals',lag.max = 5)
  pacf(lm_mrm$residuals,main = 'PACF of Residuals',lag.max = 5)
  pacf_band = 1/sqrt(46)
  pacf_band

```

```{r Seasonality}  
  
## Seasonality

  head(mrm_df)
  mrm_df$quarter = as.factor(mrm_df$quarter)
  
  # Box Plot
  boxplot(street_fees ~ quarter, data = mrm_df)
  
  # Kruskal Wallis Test # For all quearters together
  kruskal.test(street_fees ~ quarter, data = mrm_df)
  
  # So I need to perform Krushal Test only for checking difference between Q2 and Q3. 
  fil_12 = mrm_df$quarter == 'Q1'| mrm_df$quarter == 'Q2'
  fil_13 = mrm_df$quarter == 'Q1'| mrm_df$quarter == 'Q3' 
  fil_14 = mrm_df$quarter == 'Q1'| mrm_df$quarter == 'Q4'
  fil_23 = mrm_df$quarter == 'Q3'| mrm_df$quarter == 'Q2'
  fil_24 = mrm_df$quarter == 'Q4'| mrm_df$quarter == 'Q2'
  fil_34 = mrm_df$quarter == 'Q3'| mrm_df$quarter == 'Q4'
  #head(mrm_df[fil_2,])
  
  
  kruskal.test(street_fees ~ quarter, data = mrm_df[fil_12,])
  kruskal.test(street_fees ~ quarter, data = mrm_df[fil_13,])
  kruskal.test(street_fees ~ quarter, data = mrm_df[fil_14,])
  kruskal.test(street_fees ~ quarter, data = mrm_df[fil_23,])
  kruskal.test(street_fees ~ quarter, data = mrm_df[fil_24,])
  kruskal.test(street_fees ~ quarter, data = mrm_df[fil_34,])
  
  summary(lm_mrm) 
  summary(lm(street_fees ~   QoQ_CDX+ iTraxx + eur_gdp + quarter , data = mrm_df))
  summary(lm(street_fees ~   QoQ_CDX+ iTraxx + eur_gdp + D1, data = mrm_df))
  summary(lm(street_fees ~   QoQ_CDX+ iTraxx + eur_gdp + D2, data = mrm_df))
  
  no_dum_model = lm(street_fees ~   QoQ_CDX+ iTraxx + eur_gdp , data = mrm_df)
  summary(no_dum_model)
  mrm_df$errors_no_dum = predict(no_dum_model,mrm_df)
  
  kruskal.test(errors_no_dum ~ quarter, data = mrm_df[fil_12,])
  kruskal.test(errors_no_dum ~ quarter, data = mrm_df[fil_13,])
  kruskal.test(errors_no_dum ~ quarter, data = mrm_df[fil_14,])
  kruskal.test(errors_no_dum ~ quarter, data = mrm_df[fil_23,])
  kruskal.test(errors_no_dum ~ quarter, data = mrm_df[fil_24,])
  kruskal.test(errors_no_dum ~ quarter, data = mrm_df[fil_34,])
  
  
 

```


#2. Stationarity Checks

```{r}
# Stationarity Checks on Variable



# Time Trend of Each Variable - Dependent and Independent
# Time trendRegression and plot

keep_col_stationary = c('eur_gdp','iTraxx','CDX','QoQ_CDX','street_fees','SoW','t','QoQ_iTraxx')
relevent_data = subset(mrm_df, select = keep_col_stationary)

# Creating a residuals variables 
residuals = c(NA,lm_mrm$residuals)
relevent_data$residuals = residuals

# final check on relvent_data
head(relevent_data,10)
tail(relevent_data,10)


mef_list = c('eur_gdp','iTraxx','CDX','street_fees','SoW')


stationary_check =function(mef_list, relevent_data){
    n = length(rownames(relevent_data))
    for(mef in mef_list){
      
      print(mef)
      name = paste('The Plot of ', mef, ' vs time','.png',sep ='')
      print(name)
      #Making the Plot
      pl = ggplot(relevent_data,aes_string(y = mef,x ='t'))
      pl2 = pl+ geom_point() + geom_smooth(method = 'lm')
      print(pl2)
      
      # Running the Regression
      eq = paste(mef,'~ t',sep ='')
      print(eq)
      model1 = lm(eq,data =relevent_data)
      print(paste('Printing the results of Time Trend of',mef))
      print(summary(model1))
     
       ###  Doing the ADF Test
      print(paste('Doing the ADF tests on ',mef))
      max_adf_lag_length =trunc((n - 1 )^(1/3))
      
        # No Mean or Trend Stationarity
        print(paste('Checking for ADF with no mean or trend for stationarity of ',mef))
        adf_mean = ur.df(y = relevent_data[,mef], type = 'none',lags = max_adf_lag_length, selectlags = 'AIC')
        print(summary(adf_mean))
      
        # Checking for Mean stationarity
        print(paste('Checking for ADF Mean stationarity on ',mef))
        adf_mean = ur.df(y = relevent_data[,mef], type = 'drift',lags = max_adf_lag_length, selectlags = 'AIC')
        print(summary(adf_mean))
        
        #checking for trend stationarity
        print(paste('Checking for ADF Trend stationarity on ',mef))
        adf_tr = ur.df(y = relevent_data[,mef], type = 'trend',lags = max_adf_lag_length, selectlags = 'AIC')
        print(summary(adf_tr))
      
       #### PP Test
      print(paste('Doing the PP tests on ',mef))
      max_pp_lag_length = trunc(4*(n/100)^0.25)
      
        # PP Mean Stationarity
        print(paste('Checking for PP Mean stationarity on ',mef))
        pp_mean  = ur.pp(relevent_data[,mef], type = 'Z-tau',model = 'constant', lags = 'short')
        print(summary(pp_mean))
        
        # PP Trend Stationarity
        print(paste('Checking for PP Trend stationarity on ',mef))
        pp_tr  = ur.pp(relevent_data[,mef], type = 'Z-tau',model = 'trend', lags = 'short')
        print(summary(pp_tr))
        
      print(paste('Doing the KPSS tests on ',mef))
    
      #### KPSS Test
        # KPSS Mean Stationarity
        print(paste('Checking for KPSS Mean stationarity on ',mef))
        print(kpss.test(relevent_data[,mef], null = 'Level'))
        
        # KPSS Trend Stationarity
        print(paste('Checking for Trend stationarity on ',mef))
        print(kpss.test(relevent_data[,mef], null = 'Trend'))
    
    }
}

stationary_check(mef_list,relevent_data)


```



```{r}
# Doing the Stationarity Checks on Residuals and QoQ_CDX

relevent_data_2 = relevent_data[-1,]
mef_list2 = c('QoQ_CDX', 'residuals')
stationary_check(mef_list2,relevent_data_2)

```
##Stationarity Check on QoQ% Transformation of iTraxx

```{r}
# check  on iTraxx

head(mrm_df, 2)

df = mrm_df
df$QoD_iTraxx = df$QoQ_iTraxx*df$L1_iTraxx
head(df)
stationary_check('QoQ_iTraxx',relevent_data_2)

stationary_check('QoD_iTraxx',df[-1,])


lm_mrm_itraxx = lm(street_fees ~   QoQ_CDX+ QoQ_iTraxx + eur_gdp + D1 + D2 , data = mrm_df)
summary(lm_mrm_itraxx)
summary(lm_mrm)

```

### Checking for stationarity on iTraxx by removing 2008 Q1 observation
```{r}

relevent_data_3 = mrm_df[!mrm_df$time %in% c('2008 Q1','2008 Q4'),]
head(relevent_data_3,20)

stationary_check('iTraxx',relevent_data_3)
```


#3. Outlier Analysis

```{r}


#### 3. Inluential Points/Outlier Analysis ####

# 1. Leverage
# 2. Standarized Residual
# 3. Cooks Distance
# 4. Dfbeta
# 5. Dffits



# Calculating cooks distance


influence_data = as.data.frame(influence.measures(lm_mrm)$infmat)
influence_data$time = rownames(influence_data)
typeof(influence_data)
head(influence_data)



# Plotting Leverage
leverage_mean = mean(influence_data$hat)
pl = ggplot(influence_data,aes(y = hat,x = rownames(influence_data) ))
pl+ geom_col()+ theme(axis.text.x = 
                        element_text(angle = 90,
                                     vjust = 0)) + 
                    xlab('Time')+ 
                    ylab('Leverage')+ 
                    geom_hline(yintercept = 2*leverage_mean,linetype = 'dashed',color = 'red',size = 0.5) 




# Plotting the standardized or internally-studentized residuals

influence_data$studres = rstandard(lm_mrm)
pl = ggplot(influence_data,aes(y = studres,x = rownames(influence_data) ))
pl+ geom_col()+ theme(axis.text.x = 
                        element_text(angle = 90,
                                     vjust = 0)) + 
                    xlab('Time')+ 
                    ylab('Standardized Residuals')



# Plottign the Cooks distance
k = length(lm_mrm$coefficients)
n = nrow(influence_data)
cook_d_limit = qf(0.5,df1 =k,df2 = n)
cook_lab = paste("Cook's Distance = ",'F_','{',0.5,'}','(',k,',',n,')',sep = '')
pl = ggplot(influence_data,aes(y = cook.d,x = rownames(influence_data) ))
pl+ geom_col()+ theme(axis.text.x = 
                        element_text(angle = 90,
                                     vjust = 0)) + xlab('Time')+ ylab("Cook's Distance")+ 
                    #geom_hline(yintercept = cook_d_limit,linetype = 'dashed',color = 'red',size = 0.5)+
                    #geom_text(aes( 0, cook_d_limit, label = cook_lab, vjust = -1, hjust = -0.9), size = 3)+#+
                    geom_hline(yintercept = 4/(n),linetype = 'dashed',color = 'orange',size = 0.5)+
                    geom_text(aes( 0, 4/(n), label = 'Cooks Distance = 4/(n)', vjust = -1, hjust = -1), size = 3)


# Plotting Standarized residuals vs Leverage
pl = ggplot(data = influence_data,aes(x = hat,y = studres))#, label = cook.d)
pl2 = pl+ geom_point(aes(size = cook.d)) + xlab('Leverage') + ylab('Standarized Residuals')
pl2 +  geom_text(data=subset(influence_data, cook.d > 0.05),
            aes(hat,studres,label=time),
            check_overlap = T,hjust = -0.1,vjust = -0.5)


# Plotting Cook's Distance vs Leverage
pl = ggplot(data = influence_data,aes(x = hat,y = cook.d))
pl2 = pl+ geom_point() + xlab('Leverage') + ylab("Cook's distance")
pl2 +  geom_text(data=subset(influence_data, cook.d > 0.04),
            aes(hat,cook.d,label=time),
            check_overlap = T,hjust = -0.1,vjust = -0.5)#+
      #geom_hline(yintercept = 0.04,linetype = 'dashed',color = 'red',size = 0.5)

  
4/n
4/(n-k-1)
# Ploting the models
plot(lm_mrm)


```

Trying to rebuild the model after removing influential points


```{r}
(mrm_df[-9,])
lm_mrm_influence = lm(street_fees ~   QoQ_CDX+ iTraxx + eur_gdp + D1 + D2 , data = mrm_df[-9,])
summary(lm_mrm_influence)
confint(lm_mrm,level = 0.95)
summary(lm_mrm)
```

```{r}

cdx_itraxx = lm(QoQ_CDX ~ iTraxx, data = mrm_df)
summary(cdx_itraxx)
summary(lm_mrm)

ggplot(data = mrm_df, aes(x = iTraxx, y = QoQ_CDX))+ geom_point() + geom_smooth(method = 'lm')

```

# In - Sample and Out-Sample Analysis



```{r}

# Functions




```



```{r}

# Some links in helping timeries plots

# https://stackoverflow.com/questions/28679055/r-setting-limits-to-scale-x-yearqtr-in-ggplot-for-yearqtr-zoo
# http://www.sthda.com/english/articles/32-r-graphics-essentials/128-plot-time-series-data-using-ggplot/

```

### In Sample Analsysis
```{r}

str(mrm_df)

revenue_comparison_and_plot = function(start_Q,end_Q,mrm_df){
  
  # Filterig mrm_df
  start_t = mrm_df[mrm_df$time == start_Q,'t']
  end_t = mrm_df[mrm_df$time == end_Q,'t']
  fil = (mrm_df$t >= start_t &  mrm_df$t <= end_t)
  
  # Making a Plot
  df = subset(mrm_df[fil,], select = c('yearqtr','predictions','street_fees'))
  qtr_breaks = seq(from = min(df$yearqtr), to = max(df$yearqtr), by = 0.25) # Puting label for each qtr
  
  # Transforming Data Frame to Make Plots
  long_df = df %>% gather(key = 'type',value = 'revenues',-yearqtr)
  pl = ggplot(long_df, aes(x = yearqtr, y = revenues)) +
          geom_line(aes(color = type), size = 1) +
          geom_point(aes(color = type), size = 1.5)+
          scale_color_manual(values = c("red", "blue"))+  
          theme_bw()
  pl2 = pl  + scale_x_yearqtr(breaks = qtr_breaks,
                          format = '%Y Q%q')+
                          theme(axis.text.x=element_text(angle=45, vjust=0.5))
  pl3 = pl2 + ylab('Reveneus')+ xlab('Time')  
  print(pl3)
  
  # List Output
  actual = sum(mrm_df[fil,]$street_fees)
  predicted = sum(mrm_df[fil,]$predictions)
  output = vector(mode = 'list',length = 3)
  names(output) = c('predicted','actual','actual_band')
  output$actual = actual
  output$predicted = predicted
  output$actual_band = actual*1.1 # For 10% buffer on actual
  # Return
  return(output)
}



## In Sample Function
insample_analysis = function(start_Q,end_Q,mrm_df,is_mar_and_sd_out_of_filter = F){
  
  
  #print(summary(lm_mrm))
  mrm_df$predictions = predict(lm_mrm, newdata = mrm_df)
  
  # #Printing the Resulst of whole of Insample disregarding the start_Q and End_Q
  # print("Printing the Resulst of whole of Calibration Data from 2006 Q to 2017 Q3")
  # print(performance_metric(mrm_df,fil = rep(T,),is_mar_and_sd_out_of_filter = F))
  
  
  # Filterig mrm_df
  start_t = mrm_df[mrm_df$time == start_Q,'t']
  end_t = mrm_df[mrm_df$time == end_Q,'t']
  fil = (mrm_df$t >= start_t &  mrm_df$t <= end_t)
  
  #Plots
  print(revenue_comparison_and_plot(start_Q,end_Q,mrm_df))
  
  # Peformance Matric Calculation
  print(paste("Printing the result of model performance between", start_Q ,"and",end_Q))
  print(performance_metric(mrm_df,fil,is_mar_and_sd_out_of_filter))
  
}



```


### Outsample Function
```{r}

outsample_analysis = function(start_Q,end_Q,mrm_df,is_mar_and_sd_out_of_filter = T){
  
  #print(summary(lm_mrm))
  mrm_df$predictions = predict(lm_mrm, newdata = mrm_df)
  
  # Filtering
  start_t = mrm_df[mrm_df$time == start_Q,'t']
  end_t = mrm_df[mrm_df$time == end_Q,'t']
  
  outsample = (mrm_df$t >= start_t &  mrm_df$t <= end_t)
  insample = !outsample
  
  # Insample and Outsample Dataframe Creations 
  insample_df = mrm_df[insample,]
  outsample_df = mrm_df[outsample,]
  
  # Model Calibration on Insample Data
  model = lm(street_fees ~   QoQ_CDX+ iTraxx + eur_gdp + D1 + D2 , data = insample_df)
  print("Model Peformance")
  print(summary(model))
  
  # Model Prediction using Outsample Data
  mrm_df$predictions = predict(object = model, newdata = mrm_df)
  # Although this is being done for whole data but we are only interested in Outsample peformance, as we are passing outsample filter 
  
  
  #Plot
  print(revenue_comparison_and_plot(start_Q,end_Q,outsample_df))

  # Peformance Matric Calculation
  print(paste("Printing the result of model performance between", start_Q ,"and",end_Q))
  print(performance_metric(mrm_df,outsample,is_mar_and_sd_out_of_filter))
}


```


## INSAMPLE RUN
```{r}
# Insample Rune Complete
insample_analysis('2006 Q1','2017 Q3',mrm_df)

# Insample analysis of Last 8 qtrs
insample_analysis('2015 Q4','2017 Q3',mrm_df)

## STRESS PERIOD PERFORMANCE
# Insample analysis of model performance in financial crisis between 2007 Q3 and 2009 Q2
insample_analysis('2007 Q3','2009 Q2',mrm_df)
```

## OUTSAMPLE RUN
```{r}

#
# Outsample analysis of Last 8 qtrs
outsample_analysis('2015 Q4','2017 Q3',mrm_df,is_mar_and_sd_out_of_filter = T)

## STRESS PERIOD PERFORMANCE
# Outsample analysis of model performance in financial crisis between 2007 Q3 and 2009 Q2
outsample_analysis('2007 Q3','2009 Q2',mrm_df,is_mar_and_sd_out_of_filter = T)

```


### Structual Break Test

```{r}

# Series with structural break

x =1:100


x1 = x[1:50]
x2 = x[51:100]

y1 = 1+ 10*x1 + rnorm(50)
y2 = 4 + 6*x2 + rnorm(50)

y = c(y1,y2)

mm1 = lm(y ~x)
summary(mm1)

qlr = Fstats(mm1,data = data.frame(cbind(y,x)))
plot( qlr, alpha = 0.05)

sctest(y ~x , type = 'Chow',point = 50)
lines(breakpoints(qlr),col = 'red',lwd = 2)

plot(x,y)

```



```{r}
# Doing the Test on model data

qlr = Fstats(lm_mrm,data = mrm_df)
plot(qlr,alpha = 0.05)
sctest(qlr , type = 'supF')
lines(breakpoints(qlr),col = 'red',lwd = 2)

qf(0.95,df1 = 6,df2 = 46-12 )
qf(0.95,df1 = 6,df2 = 34 )
qf(0.99,df = 2,df2 = 22)


anova(lm_mrm)
qf(0.95,df1 = 12,df2 = 10 )

breakpoints(qlr)

```


# Stability Test

```{r}




# MegaFunction 
stability_analysis = function(mrm_df,cut_off_percentage = 0.3, confidence_level = 0.95){
  # Adapated from Shipra's Code 
  # Checking the Number of Points we have after removing NA obsrvation
  stability_df = mrm_df %>% drop_na()
  #head(stability_df)
  #tail(stability_df)
  
  
  # Running Regression
  lm_stability = lm(street_fees ~   QoQ_CDX+ iTraxx + eur_gdp + D1 + D2 , data = stability_df)
  summary(lm_stability)
  
  # Coefficient Dataframe
  coefficient_df = as.data.frame(summary(lm_stability)$coefficients)
  coefficient_df
  
  # Extracting Variables for Test
  variable_list = rownames(coefficient_df)
  variable_list
  
  # Calculating No. of Points to Run Stability Test

  # Sample Size = n
  n = dim(stability_df)[1]
  #n
  
  # Initital Sample observation number from which 
  round(n*cut_off_percentage)
  ceiling(n*cut_off_percentage)
  
  n_forward_stability = n - ceiling(n*cut_off_percentage) # Ceiling over round as ceiling round to highest nearest integer 
  n_forward_stability # So Forward stability includes observations from  1 to n_forward_stability = 32
  
  n_backward_stability = ceiling(n*cut_off_percentage)
  n_backward_stability # So Backward stability starts from n_backward_stability + 1(= 15) to n (= 46)
  
  
  # Mega Loop Begins 
  for(var in variable_list){
    paste0("Stability Analysis for ",coefficient_df[var,1])
      
            # Confidence Inteval
            confidence_interval_var = confint(lm_stability,parm = var, level = confidence_level)
            beta_l = rep(confidence_interval_var[1],n)
            beta_u = rep(confidence_interval_var[2],n)
          
            # Creating vectors for beta_forwards and beta_backwards
            beta_forwards = rep(NA,n)
            beta_backwards = rep(NA,n)
            
            
            #### Forward Stability ####
            
            # Forward Model Calibration on 1:n_forward_stability
            model_forward = lm(street_fees ~   QoQ_CDX+ iTraxx + eur_gdp + D1 + D2 , data = stability_df[1:n_forward_stability,])
            # summary(model_forward)
            coeff_forward = as.data.frame(summary(model_forward)$coefficients)
            beta_hat_forward = coeff_forward[var,1]
            
            # Assigning Beta_hat_foward on array of beta_forwards
            #beta_forwards[1:n_forward_stability] = beta_hat_forward
            beta_forwards
            
            ### Calculation
            for(i in ( n_forward_stability+1 ):n ){
                forward_model = lm(street_fees ~   QoQ_CDX+ iTraxx + eur_gdp + D1 + D2 , data = stability_df[1:i,])
                incremental_coeff_forward = as.data.frame(summary(forward_model)$coefficients)
                new_beta_forward = incremental_coeff_forward[var,1] 
                beta_forwards[i] = new_beta_forward
            }
            
            
            
            #### Backward Stability ####
            
            n_backward_stability
            # Backward Model Calibration on n_backward_stability:n
            model_backward = lm(street_fees ~   QoQ_CDX+ iTraxx + eur_gdp + D1 + D2 , data = stability_df[ (n_backward_stability+1):n,])
            #summary(model_backward)
            coeff_backward = as.data.frame(summary(model_backward)$coefficients)
            beta_hat_backward = coeff_backward[var,1]
            
            # Assigning Beta_hat_foward on array of beta_backwards
            #beta_backwards[(n_backward_stability+1):n] = beta_hat_backward
            #beta_backwards
            
            ### Calculation
            for(i in n_backward_stability:1 ){
                backward_model = lm(street_fees ~   QoQ_CDX+ iTraxx + eur_gdp + D1 + D2 , data = stability_df[i:n,])
                incremental_coeff_backward = as.data.frame(summary(backward_model)$coefficients)
                new_beta_backward = incremental_coeff_backward[var,1] 
                beta_backwards[i] = new_beta_backward
            }
            
            
            
            
            #### Plotting the Combined Plot
            
            # Subsetting a dataframe
            df = subset(stability_df, select = c('yearqtr'))
            df$beta_forwards = beta_forwards
            df$beta_backwards = beta_backwards
            df$beta_l = beta_l
            df$beta_u = beta_u
            #df
           
            # Making Qtr Breaks for The Data            
            qtr_breaks = seq(from = min(df$yearqtr), to = max(df$yearqtr), by = 0.25) # Puting label for each qtr
            
            # Transforming Data Frame to Make Plots
            long_df = df %>% gather(key = 'type',value = 'Beta',-yearqtr)
            long_df
            pl = ggplot(long_df, aes(x = yearqtr, y = Beta)) + 
                            geom_line(aes(color = type), size = 1) + 
                            geom_point(aes(color = type,shape = type), size = 2) + 
                    scale_color_manual(values = c("blue","green", "orange",'red'))+ scale_shape_manual(values=c(1,4, 6, 8)) + theme_bw()
            pl2 = pl  + scale_x_yearqtr(breaks = qtr_breaks,
                                    format = '%Y Q%q')+
                                    theme(axis.text.x=element_text(angle=90, vjust=0.5))
            pl3 = pl2 + ylab(paste('Coefficient estimates of',var))+ xlab('Time') + theme(panel.grid.major = element_line(size = 0.2, linetype = 'dashed',
                                colour = "black") )
            print(pl3)
            
            plot_path = file.path("M:\\Model Validation\\IBCM EMEA DCM Model Validation\\R Code\\Report Pictures\\stability",paste0("Stability","_",var,".png"))  
            ggsave(plot_path, plot = last_plot(),dpi = 300,width = 12,height = 6)
          
  }
  
}
```


```{r}

stability_analysis(mrm_df)

```




# Senstivity Analsys
```{r}
# Just checking the code for 


transform_data = function(baseline_df,cut_off_quarter){
     
      cut_off_quarter = as.yearqtr(cut_off_quarter,format = "%Y Q%q")
     # Changing the Names
      colnames(baseline_df) = c('time', 'eur_gdp','iTraxx',
                           'CDX','D1','D2')
      
      # Splitting the time column in Quarter and Year
      temp = str_split_fixed(baseline_df$time,' ',2)
      baseline_df$quarter = temp[,1]
      baseline_df$year = as.numeric(temp[,2])
      baseline_df$time = paste(baseline_df$year, baseline_df$quarter)
      baseline_df$quarter = NULL
      baseline_df$year  = NULL
      
      # Creation of QoQ on CDX
      baseline_df$L1_CDX = Lag(baseline_df$CDX,shift = 1)
      baseline_df$QoQ_CDX = mapply(growth_transform, baseline_df$CDX, baseline_df$L1_CDX)
      
      
      
      # Creating yearqtr Variable
      baseline_df$time = as.yearqtr(baseline_df$time,format = "%Y Q%q")
      
      # Changing the row names to time variable
      row.names(baseline_df) = baseline_df$time
      
      
      return(baseline_df[baseline_df$time >= cut_off_quarter,])
}

data_input_output = function(cut_off_quarter = '2017 Q4'){
  baseline_df = as.data.frame(read.csv('emea_dcm_baseline.csv'))
  china_df = as.data.frame(read.csv('emea_dcm_china.csv'))
  euro_df = as.data.frame(read.csv('emea_dcm_euro.csv'))
  sftq_df = as.data.frame(read.csv('emea_dcm_sftq.csv'))
  emhl_df = as.data.frame(read.csv('emea_dcm_emhl.csv'))
  
  baseline_df = transform_data(baseline_df, cut_off_quarter)
  euro_df = transform_data(euro_df,cut_off_quarter)
  china_df = transform_data(china_df,cut_off_quarter)
  sftq_df = transform_data(sftq_df,cut_off_quarter)
  emhl_df = transform_data(emhl_df,cut_off_quarter)
  
  
  return(list(baseline_scenario=baseline_df,
              euro_scenario= euro_df,
              china_scenario =china_df,
              sftq_scenario = sftq_df,
              emhl_scenario = emhl_df))
}




partial_derivatives_calculation = function(mrm_df, keep_indpt_vars = c('eur_gdp','iTraxx','CDX','D1','D2'),
                                  critical_pvalue = 0.05){
  # Subsetting dataframe
  loop_reg_df = subset(x = mrm_df, select = keep_indpt_vars)
  
  # Creating or initiating empty dataframe for adding of loop
  df = data.frame(matrix(nrow = 6,ncol = 0), row.names = c('(Intercept)',keep_indpt_vars) )

  for(var in keep_indpt_vars){
    ols_equation = as.formula( paste(var,"~ .",sep = "")   ) #QoQ_CDX is out at it is not changed but CDX is
    model = lm(ols_equation, data = loop_reg_df)
    
    #Extracting the coefficient with p value and standard errors
    coeff_df = data.frame(summary(model)$coefficients)
    
    # A Extracting only the statistically significant coefficients
    coeff_df[,var] = mapply(function(e,p){    if(p > critical_pvalue){ return(0) } else{ return(e) }   }, 
                               coeff_df$Estimate,coeff_df$Pr...t.. )
    
    
    estimate_df = coeff_df[,var,drop = FALSE]
    estimate_df
    
    df = merge(df,estimate_df,by = "row.names", all = T)
    rownames(df) = df$Row.names
    df$Row.names = NULL
  }
  # Changing NA Values to 1 as we are outputting partial derivatives
  df[is.na(df)] = 1
  
  return(df)
}



sensitivity_analysis = function(var,df,n_shock_quarter =1,
                                varlist = c('eur_gdp','iTraxx','CDX'),
                                calibration_df= mrm_df,
                                model = lm_mrm,
                                method = 'independent',
                                critical_p_value = 0.05){
  
  #var = shocked variable
  #varlist = 
  #df = dataframe after 2017 Q3
  #mrm_df = dataframe till 2017 Q3
  #lm_mrm = linear regression model
  #
  # Method = independnet means variables or MEFs are independently shocked one at a time where as
  # Method = partial_derivative, calculate the change in other MEFS given a change in the MEF by regressing one mef from other MEFs on
  # rest of the other MEFs and the MEF.
  #
  #
  #
  # For example say we shocking CDX and the other MEFs = c( iTraxx, eur_gdp), then we can see the effect of unit change of CDX
  # on eur_gdp and iTraxx as follows
  #
  # iTraxx = beta_0 + beta_c*CDX + beta_e*eur_gdp, here beta_c gives the effect of unit change in CDX on iTraxx
  # given a level of eur_gdp
  #
  # eur_gdp = alpha_0 + alpha_c*CDX + alpha_t*itraxx,here alpha_c givens the effect of unit change in CDX on eur_gdp 
  #given a level of itraxx

  
  
  
  
  # Creating a lagging function
  lag_CDX = function(df){
      n = dim(df)[1]
      for(i in 2:n){
        df$L1_CDX[i] = df$CDX[i-1]
      }
      return(df)
  }
  
  # To find out the absolute value and preserving its sign
  absmax <- function(x) { 
  # converting list to vectors
      x = unlist(x,use.names = FALSE)
    return(x[which.max( abs(x) )])
  }

  # Method asign
  if(method == "independent"){
    print(paste('The method is',method))
    partial_derivatives_df = setNames(data.frame(diag(x = 1, nrow = 3, ncol = 3 ), row.names = varlist), varlist)
  } else if(method == "partial_derivative"){
    print(paste('The method is',method))
    partial_derivatives_df = partial_derivatives_calculation(calibration_df,
                                                             critical_pvalue = critical_p_value) #90% Confidence
  } else{
    print("NO RIGHT METHOD DEFINED, PROGRAMS ENDS")
    return(NA)
  }
  
  # No Shock Revenues
  df$baseline_revenues = predict(model, df)

  # QoQ Series on Var used by MD
  qoq_var = qoq_p(  mrm_df[,var] )
  qoq_var = qoq_var[-1] #removing the first NA observations
  
  # Creatign Shock Indicator and Shock Factor
  df$shock_indicator = c(rep(1,n_shock_quarter),rep(0,8-n_shock_quarter))
  shock_types = c("S1: +10% Shock","S2: -10% Shock","S3: +1SD","S4: -1SD","S5: 5 percentile",
                  "S6: 10 percentile","S7: 90 percentile","S8: 95 percentile")
  shock_factor =list("S1: +10% Shock" =0.1, "S2: -10% Shock" = -0.1, "S3: +1SD" = sd(calibration_df[,var]) ,
                     "S4: -1SD"=  -sd(calibration_df[,var]),
                     "S5: 5 percentile" = quantile( qoq_var ,0.05  ),
                     "S6: 10 percentile" = quantile( qoq_var,0.1 ),
                     "S7: 90 percentile" = quantile( qoq_var, 0.9 ),
                     "S8: 95 percentile" = quantile(qoq_var,0.95)
                     )
  
  # Creating Temporary version of df so that is value are not altered in loop
  temp_df = df
  Max_delta_vs_baseline= list()
  total_delta_vs_baseline = list()
  
  for(shock in shock_types){
    # Creating Shock
    
    
    # Conditiions due to as percentage shocks are multiplicative while sd shocks are additive
    if(shock %in% c("S3: +1SD","S4: -1SD")){
      df[paste('change in',var,'for',shock)] = df$shock_indicator * shock_factor[[shock]]
      
    } else{
      df[paste('change in',var,'for',shock)] = df[,var] *  df$shock_indicator * shock_factor[[shock]]
    }
    
    # Applying the Shock to Variables in
    for (v in  varlist){
        temp_df[v] = df[v] + partial_derivatives_df[var,v] * df[paste('change in',var,'for',shock)]
    }
    
    
    # Lagging CDX and Calculating QoQ_CDX to predict the revenues after shock
    temp_df =lag_CDX(temp_df)
    temp_df$QoQ_CDX = growth_transform(temp_df$CDX,temp_df$L1_CDX)
    
    df[paste('Revenues for',shock)] = predict(model,temp_df)
    
    Max_delta_vs_baseline[  paste('Revenues for',shock) ] =  absmax( df[paste('Revenues for',shock)]/df$baseline_revenues - 1 )
    total_delta_vs_baseline[  paste('Revenues for',shock) ] =  sum(df[paste('Revenues for',shock)])/sum(df$baseline_revenues) - 1 
  }
  
  
  #### Making a Plot of Revenues Under Different  Shock Types
  # Creating a Dataframe
  print_df = subset( df, select = c('time','baseline_revenues',paste('Revenues for',shock_types) )   )
  
  # Making Qtr Breaks for The Data            
  qtr_breaks = seq(from = min(print_df$time), to = max(print_df$time), by = 0.25) # Puting label for each qtr
  
  # Transforming Data Frame to Make Plots
  long_df = print_df %>% gather(key = 'type',value = 'Revenue',-time)
  color_palette <- c("Red","#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
  
  pl = ggplot(long_df, aes(x = time, y = Revenue)) + 
                  geom_line(aes(color = type), size = 1) + 
                  geom_point(aes(color = type), size = 2) + 
                  scale_color_manual(values = color_palette) + theme_bw()
  
  pl2 = pl  + scale_x_yearqtr(breaks = qtr_breaks,
                          format = '%Y Q%q')+
                          theme(axis.text.x=element_text(angle=45, vjust=0.5))
  pl3 = pl2 + ylab(paste('Projected Revenue for shocks in ',var))+ xlab('Time') + 
              theme(panel.grid.major = element_line(size = 0.2, linetype = 'dashed',
                      colour = "black") )
  print(pl3)
  
  
  # Calculating the 
  
  #Printing the Results
  result = t(df[,-1])[-4:-7,] # column
  result = result[order(row.names(result)),]
  result = as.data.frame(result)
  # Ordering the Answers
  
  
  
  # Returning df, for further task if needed
  
  # # Reodering the df by colnames
  # df = df[,-1:-8]
  # df = df[,order(names(df))]
  
  
  # Creating the Max_delta_vs_baseline_df for comparison
  Max_delta_vs_baseline_df <- data.frame( matrix(unlist(Max_delta_vs_baseline), 
                                             nrow= length(Max_delta_vs_baseline), byrow=T), 
                                      row.names = names(Max_delta_vs_baseline) )
  colnames(Max_delta_vs_baseline_df) = 'max(Delta - Baseline) in a PQ'
  
  # Creating the total_delta_vs_baseline_df for comparison
  total_delta_vs_baseline_df <- data.frame( matrix(unlist(total_delta_vs_baseline), 
                                             nrow= length(total_delta_vs_baseline), byrow=T), 
                                      row.names = names(total_delta_vs_baseline) )
  colnames(total_delta_vs_baseline_df) = 'Delta vs Baseline (8Q Total)'
  
  
  result = merge(result,Max_delta_vs_baseline_df,by = 0,all = T)
  rownames(result) = result$Row.names
  result$Row.names = NULL
  
  # I know this is stupid, but I am very frustrated anyway
  result = merge(result,total_delta_vs_baseline_df,by = 0,all = T)
  rownames(result) = result$Row.names
  result$Row.names = NULL
  result = result[order(row.names(result)),]
  result = result[c(21,1:20),] # Putting Shock Indicator on Top
  print(result)
  return(result)
  

}


```




## Senstivity Analysis
```{r}
do_sensitivity_analysis = function(cut_off_quarter = "2017 Q4", quarters_to_shock =1){
  mega_df= data_input_output(cut_off_quarter = "2017 Q4")
  for(i in 1:length(mega_df)){
    for(variable in c('CDX','iTraxx','eur_gdp')){
      for(method in c('independent','partial_derivative')){
          print( paste('Conducting Senstivitiy Analysis for shocks in ',variable,
                       'using the shock interaction method as',method,
                       'on the dataframe of ', names(mega_df)[i] )  )
          
          folder_path = "M:\\Model Validation\\IBCM EMEA DCM Model Validation\\R Code\\Report Pictures\\sensitivity\\"
          file_path = paste0("dataframe is ",names(mega_df)[i],
                                        "_method is ",method,
                                        "_ variable is ",variable)
          
          #Writing the results(df) into a csv  file        
          df =sensitivity_analysis(var = variable,df = mega_df[[i]],n_shock_quarter = quarters_to_shock,method = method)
          csv_save_path = paste0(folder_path,file_path,'.csv')
          write.csv(df,file = csv_save_path )
          
          # Saving the Graphs in the folder 
          plot_path = paste0(folder_path,file_path,".png")
          ggsave(plot_path, plot = last_plot(),dpi = 300,width = 12,height = 6)
      }  
    }
  }
  # Printing out the partial derivative matrix
  partial_derivatives_calculation(mrm_df,critical_pvalue = 0.05)
}



```


# Conducting Senstivity Analsys
```{r}
do_sensitivity_analysis(quarters_to_shock = 1)
partial_df = partial_derivatives_calculation(mrm_df,critical_pvalue = 0.05)
write.csv(partial_df, 'partial_derivative_methodology.csv')

# 
# write.csv(summary(lm_mrm)$coefficients, file ='cieff.csv')
# 
# mega_df= data_input_output(cut_off_quarter = "2017 Q4")
# s =sensitivity_analysis(var = "iTraxx", df = mega_df[[1]],method = 'independent')
# d =sensitivity_analysis(var = "", df = mega_df[[1]],method = 'partial_derivative')

```


```{r}

## Finding 


negative_street_fees = function(n = 1e6, 
                                eur_gdp_l = -50, eur_gdp_h = 50, 
                                QoQ_CDX_l = -50, QoQ_CDX_h = 50,
                                iTraxx_l = 0, iTraxx_h = 2000,
                                model = lm_mrm,
                                data = mrm_df,
                                jop = "2017 Q3"){
  
  #Starting the Timer
  start_time = Sys.time()
  
  # # Assigning the Domain
  # domain_eur_gdp = seq(from = eur_gdp_l , to = eur_gdp_h, length.out = n)
  # domain_QoQ_CDX = seq(from = QoQ_CDX_l , to = QoQ_CDX_h, length.out = n)
  # domain_iTraxx = seq(from = iTraxx_l , to = iTraxx_h, length.out = n)

  # Assigning the coefficients
  beta_intercept = model$coefficients[1][[1]]
  beta_QoQ_CDX = model$coefficients[2][[1]]
  beta_iTraxx = model$coefficients[3][[1]]
  beta_eur_gdp = model$coefficients[4][[1]]
  beta_D1 = model$coefficients[5][[1]]
  beta_D2 = model$coefficients[6][[1]]
  
  # Extracting the Model Coefficient
  equation_dcm = function(QoQ_CDX, iTraxx, eur_gdp,D1, D2){
    return( beta_intercept + 
              beta_QoQ_CDX*QoQ_CDX + 
              beta_iTraxx*iTraxx+
              beta_eur_gdp*eur_gdp + 
              beta_D1*D1 + beta_D2*D2)
  }

  # Sampling the Dataframe from uniform distribution
  df = data.frame(QoQ_CDX = runif(n,min = QoQ_CDX_l,max = QoQ_CDX_h),
                    iTraxx = runif(n,min = iTraxx_l, max = iTraxx_h),
                    eur_gdp = runif(n, min = eur_gdp_l,max = eur_gdp_h)
                    )
  
  
  # Checing the Model predicted values from generated dataframe
  df$yhat = mapply(equation_dcm, df$QoQ_CDX,df$iTraxx,df$eur_gdp,0,0)
  
  # Filtering out Values where model predicts negative revenue
  solution_df = df[df$yhat <= 0,]
  # head(solution_two_df,1000)
  
  # Extracting Jump of point value from data
  jop = as.yearqtr(jop, format = "%Y Q%q")
  x_jop = subset(data[data$time == jop,], select = c('QoQ_CDX', 'iTraxx', 'eur_gdp'))
  #print(x_jop)
  
  
  # Calculating the percentage of shocks in MEFs in solution dataset compared to jump of point MEFs
  solution_df$bump_QoQ_CDX = (solution_df$QoQ_CDX/x_jop$QoQ_CDX - 1)
  solution_df$bump_iTraxx = (solution_df$iTraxx/x_jop$iTraxx - 1)
  solution_df$bump_eur_gdp = (solution_df$eur_gdp/x_jop$eur_gdp - 1)
  
  # Plotting the bumped up values
  pl = ggplot(data = solution_df, aes(x = bump_QoQ_CDX))+ geom_histogram(binwidth = 10)
  print(pl)
  #
  
  #Ending Timer
  end_time = Sys.time()
  # Printing the Timer difference
  print(paste('The implementation time taken is  ',end_time - start_time))
  return(solution_df)
}



```



```{r}






# 
# loadlibraries()

# Further Analysis
s = negative_street_fees(n = 1e6)

# head_s = head(s, 1000)
# 
# head(head_s)


# CHecking for Non Sense Observation With Regard to jump of point
jop_bump = 1
s[ abs(s$bump_QoQ_CDX) < jop_bump & abs(s$bump_iTraxx) < jop_bump & abs(s$bump_eur_gdp) < jop_bump,]

check_for_unusual = function( test_df,df = mrm_df,
                               band = TRUE,historical_band_multiplier =0.1,
                               QoQ_CDX_band, iTraxx_band, eur_gdp_band){
  # test_df = head(s,20)
  # df = mrm_df
  # band = TRUE
  # historical_band_multiplier= 100
 
  if(band == TRUE){
  # Getting Historical Data MinMax
    min_num = function(x,p){ x - abs(p*x)} # reduce x by p
    max_num = function(x,p){ x + abs(p*x)} # increase x by p
    
    min_QoQ_CDX = min_num( min(df$QoQ_CDX,na.rm = T) , historical_band_multiplier )
    max_QoQ_CDX = max_num( max(df$QoQ_CDX,na.rm = T) , historical_band_multiplier)
    
    min_eur_gdp = min_num( min(df$eur_gdp) ,historical_band_multiplier) 
    max_eur_gdp = max_num( max(df$eur_gdp) ,historical_band_multiplier)
    
    min_iTraxx = min_num( min(df$iTraxx) , historical_band_multiplier)
    max_iTraxx = max_num( max(df$iTraxx) ,historical_band_multiplier)
    
  } else {
    min_QoQ_CDX = QoQ_CDX_band[1]; max_QoQ_CDX = QoQ_CDX_band[2]
    
    min_eur_gdp = eur_gdp_band[1]; max_eur_gdp = eur_gdp_band[2]
    
    min_iTraxx = iTraxx_band[1]; max_iTraxx = iTraxx_band[2]
  }
  print(paste('min_QoQ_CDX =',min_QoQ_CDX))
  print(paste('max_QoQ_CDX =',max_QoQ_CDX))                
  print(paste('min_eur_gdp =',min_eur_gdp))                
  print(paste('max_eur_gdp =',max_eur_gdp))
  print(paste('min_iTraxx =',min_iTraxx))
  print(paste('max_iTraxx =',max_iTraxx))
  
  
  # Creating Filters
  fil_QoQ_CDX = (test_df$QoQ_CDX <= max_QoQ_CDX) & (test_df$QoQ_CDX >= min_QoQ_CDX)
  fil_eur_gdp = (test_df$eur_gdp <= max_eur_gdp) & (test_df$eur_gdp >= min_eur_gdp)
  fil_iTraxx = (test_df$iTraxx <= max_iTraxx) & (test_df$iTraxx >= min_iTraxx)
  
  # fil_QoQ_CDX
  
  #Returning output
  res = test_df[ fil_QoQ_CDX & fil_eur_gdp & fil_iTraxx,]
  print(paste('The number of observations are',dim(res)[1]))
  return(head(res,20))
}

head(s,20)

summary(s)

dim(s)


check_for_unusual(test_df = s,
                  band = TRUE,historical_band_multiplier = 0,
                  QoQ_CDX_band = c(-1,1),
                  iTraxx_band = c(0,500),
                  eur_gdp_band = c(-20,10))


```

```{r model.fail.coordinates}
model_fail_coordinates_finder = function(model,
                                        QoQ_CDX_band = c(-50,50),
                                        iTraxx_band = c(0,2000),
                                        eur_gdp_band = c(-50,50),
                                        grid_points_number = 100,
                                        D1 = 0, D2 = 0,
                                        phi, theta){
  
  # Meetings
  coefficients_df = data.frame(summary(model)$coefficients)
  
  # Assiginng Beta
  beta_intercept = coefficients_df['(Intercept)','Estimate']
  beta_QoQ_CDX = coefficients_df['QoQ_CDX','Estimate']
  beta_iTraxx= coefficients_df['iTraxx','Estimate']
  beta_eur_gdp = coefficients_df['eur_gdp','Estimate']
  beta_D1 = coefficients_df['D1','Estimate']
  beta_D2 =coefficients_df['D2','Estimate']
  
  
  # Defining iTraxx finder
  
  iTraxx_finder = function(QoQ_CDX, eur_gdp, Dum1 = D1, Dum2 = D2){
    return(  -1/beta_iTraxx * (beta_intercept+ 
                                 beta_QoQ_CDX* QoQ_CDX +
                                 beta_eur_gdp* eur_gdp+
                                 beta_D1*Dum1+
                                 beta_D2*Dum2)   )
  }
  
  QoQ_CDX_grid = seq(from = QoQ_CDX_band[1], to = QoQ_CDX_band[2], length.out = grid_points_number)
  eur_gdp_grid = seq(from = eur_gdp_band[1], to = eur_gdp_band[2], length.out = grid_points_number)
  
  #Initializig Dataframes
  df = setNames(data.frame(matrix(nrow = 0, ncol = 4)),c('QoQ_CDX','eur_gdp','D1','D2'))
  
  # Building Dataframe by for loop
  for(QoQ_CDX in QoQ_CDX_grid){
    temp_df = data.frame('QoQ_CDX' = QoQ_CDX, 'eur_gdp' = eur_gdp_grid,'D1' = D1, 'D2' = D2)
    df = bind_rows(df, temp_df)    
  }  
  
  # Calculaitng iTraxx Values
  df$iTraxx = mapply(iTraxx_finder,df$QoQ_CDX,df$eur_gdp,df$D1,df$D2)
  df = df[df$iTraxx >= iTraxx_band[1] & df$iTraxx <= iTraxx_band[2],]
  
  scatter3D(x = df$QoQ_CDX, y = df$eur_gdp,z = df$iTraxx,
            cex = 0.5, pch = 1,phi = phi, theta = theta,
            ticktype = 'detailed',bty = 'g')
  return(df)
}

df = model_fail_coordinates_finder(model = lm_mrm,grid_points_number = 500,
                              phi = 5, theta = 55)
head(df)
# par(mar=c(1,1,1,1))



```




# Scenario Projection Analysis
```{r scenario.function}
####
data_input_output("2017 Q4")

#  Plotting the results 
scenario_projection_analysis = function(df = mrm_df,start_t = "2008 Q1", end_t = "2009 Q4" ,
                                        model = lm_mrm,
                                        crisis_name = "US Stress Period",
                                        cut_off_q = "2017 Q4"){


  
  
  # filtering relevent df values
  start_t = as.yearqtr(start_t, format = "%Y Q%q")
  end_t = as.yearqtr(end_t, format = "%Y Q%q")
  df$time = as.yearqtr(df$time,format = "%Y Q%q" )
  
  
  # Getting the Stress DFs
  mega_df= data_input_output(cut_off_quarter = cut_off_q)
  # mega_df
  
  
  # Making Revenue Projection and Plotting
  temp_df = subset(df[df$time >= start_t & df$time <= end_t,], select = c('time','street_fees'))
  colnames(temp_df) = c('time',paste(crisis_name,"Street fees"))
  # temp_df
  temp_df['LPA_Euro Street fees']  = predict(model,mega_df$euro_scenario)
  temp_df['LPA_China Street fees'] = predict(model, mega_df$china_scenario)

  
  revenue_plot = draw_time_series_plot(temp_df, y_label = 'Projected Revenue in different scenarios',
                                       legend = 'right')

 
  # Plotting MEFs as well
  var_list = c('CDX','QoQ_CDX','iTraxx','eur_gdp')
  for(mef in var_list){
       temp_df = subset(df[df$time >= start_t & df$time <= end_t,], select = c('time',mef))
       colnames(temp_df) = c('time',paste(crisis_name,mef) )
       temp_df[paste0('LPA_Euro ',mef)] = mega_df[[2]][,mef]
       temp_df[paste0('LPA_China ',mef)] = mega_df[[3]][,mef]
       plot = draw_time_series_plot(temp_df, y_label = paste( mef,'in different scenarios'))
       assign(paste0(mef,'_plot'),plot)
  }
  
  grob_to_plot = arrangeGrob(revenue_plot,CDX_plot,QoQ_CDX_plot,iTraxx_plot,eur_gdp_plot,
              widths = c(1, 1),
              layout_matrix = matrix(c(1,1,2,3,4,5), nrow=3, byrow=TRUE)
              )
  

  
  folder_path = "M:\\Model Validation\\IBCM EMEA DCM Model Validation\\R Code\\Report Pictures\\scenario\\"
  file_path = paste0("Scenario Analysis in ",crisis_name,' from ',start_t,' to ',end_t)
  
  # # Saving the Graphs in the folder 
  plot_path = paste0(folder_path,file_path,".png")
  ggsave(plot_path, plot = grob_to_plot,dpi = 300,width = 12,height = 10)        


  
  #Plotting the MEFs as well
  #return(revenue_plot)
}
```

```{r scenario}

scenario_projection_analysis(start_t = '2011 Q2',end_t = '2013 Q1', crisis_name = "Eurozone Crisis")
scenario_projection_analysis(start_t = '2012 Q2',end_t = '2014 Q1', crisis_name = "Eurozone 2 Crisis")




```




```{r Bootstrapping.Beta.Coefficient}

# Benchmarking Log Stage

# Model Checking 

# LoG Model

head(mrm_df)

raptr_df = subset(mrm_df, select = c('eur_gdp','iTraxx','CDX','D1','D2','street_fees','QoQ_CDX'))
raptr_df$log_street_fees = log(raptr_df$street_fees)
raptr_df$street_fees = NULL

write.csv(raptr_df, file = 'log_street_fees_model.csv',row.names = FALSE)
head(raptr_df)

# Implementation Checks
model_log_street_fees = lm(log(street_fees) ~   QoQ_CDX+ iTraxx + eur_gdp + D1 + D2 , data = mrm_df)
summary(model_log_street_fees)


# Doing Stepwise Regression
runif(30, min = 1, max = 47)
# bootstrapping_check_model= function(no_of_simulations = 10,data = mrm_df, model = lm_mrm){
#   # Meetings
#   coefficients_df = data.frame(summary(model)$coefficients)
#   
#   # Assiginng Beta
#   beta_intercept = coefficients_df['(Intercept)','Estimate']
#   beta_QoQ_CDX = coefficients_df['QoQ_CDX','Estimate']
#   beta_iTraxx= coefficients_df['iTraxx','Estimate']
#   beta_eur_gdp = coefficients_df['eur_gdp','Estimate']
#   beta_D1 = coefficients_df['D1','Estimate']
#   beta_D2 =coefficients_df['D2','Estimate']
#   
#   
#   
#   for(i in 1:no_of_simulations){
#     temp_model = lm
#   }
#   
#   return(beta_QoQ_CDX)
#   
# }

# bootstrapping_check_model()



```

`


```{r benchmarking}

combin = function(n,r){
    factorial(n)/(factorial(n-r)*factorial(r))
}

permute = function(n,r){
  factorial(n)/(factorial(n-r))
  }



# Making a dataframe of Variable to Test for Benchmarking

gen_bench_df = function(start_t = '2006 Q1'){
  bench_df = data.frame(read_xlsx(path = 'Data for Variable Selection.xlsx',sheet = 'Full Variable List'))
  street_fees_df = setNames(   data.frame(read_xlsx(path = 'Data for Variable Selection.xlsx',sheet = 'DCM Street Fees'))  ,
                          c('Quarter','street_fees'))
# 
#   start_t = '2006 Q1'
  bench_df$street_fees = street_fees_df$street_fees
  bench_df$time = as.yearqtr(bench_df$Quarter)
  
  D1_fun = function(x){
    y = as.numeric( format(x, format = "%q") )
    return( ifelse( y == 1, 1, 0) )
  }
  
  D2_fun = function(x){
    y = as.numeric( format(x, format = "%q") )
    return( ifelse( y == 2, 1, 0) )
  }
  

  bench_df$D1 = sapply(bench_df$time,FUN =  D1_fun)
  bench_df$D2 = sapply(bench_df$time,FUN =  D2_fun)
  
  
  bench_df = bench_df[c('time','street_fees','D1','D2',colnames(bench_df[,1:99]))]
  bench_df$Quarter = NULL
  row.names(bench_df) = bench_df$time


  # Removing observations before 2006 Q1
  start_t = as.yearqtr(start_t)
  bench_df =bench_df[bench_df$time >= start_t, ]
  bench_df$time =NULL

  return(bench_df)
}

gen_seg_df = function(type = "All"){
  df = gen_bench_df()
  cutoff_t = as.yearqtr('2006 Q1')
  corp_fig_df = as.data.frame(read_xlsx('corporate_fig.xlsx',sheet = 'Summary'))
  corp_fig_df$time = as.yearqtr(corp_fig_df$Quarter,format = '%Y Q%q')
  corp_fig_df$Quarter = NULL
  # str(corp_fig_df)
  
  corp_fig_df = corp_fig_df[corp_fig_df$time >= cutoff_t,]
  # head(corp_fig_df)
  # tail(corp_fig_df)
  
  if(type == 'FIG'){
    df$street_fees = corp_fig_df$`DCM Street FIG`[1:nrow(df)]
    print('The street fees are of FIG')
  } else if (type == 'CORP'){
    df$street_fees =  corp_fig_df$`DCM Street Corp`[1:nrow(df)]
    print('The street fees are of CORP')
  }
  
  return(df)
}

# head(gen_corp_df('CORP')[,1:4],5)

benchmarking = function(df ,
                        no_of_factor = 3,
                        minimum_cor2_percentile = 0.5,
                        minimum_adjr2 = 0.8,
                        irrelevent_variables = c('iTraxx_5yr_senior'),
                        transformations_list = list('QoQ_D' = qoq,'QoQ_P' = qoq_p,'L1' = L1 ),
                        dummy_vec = c('D1','D2'),
                        dependent_var = 'street_fees',
                        parallel_computing = TRUE){
  
                        # df = gen_bench_df()
                        # no_of_factor = 2
                        # minimum_cor2_percentile = 0.5
                        # irrelevent_variables = c('iTraxx_5yr_senior')
                        # transformations_list = list('QoQ_D' = qoq,'QoQ_P' = qoq_p)
                        # dummy_vec = c('D1','D2')
                        # dependent_var = 'street_fees'
                        # parallel_computing = F

  
  #Removing irrelevent variables
  df = df[,!colnames(df) %in% irrelevent_variables]
  
  #Extracting Dummy To seperate datasets
  dummy_df = subset(df, select = c('D1','D2'))
 
  #Dropping Dummys from df
  df$D1 = NULL
  df$D2 = NULL
  
  # Applying Transformations
  print('Conserding following Transformations')
  print(names(transformations_list))
  
  transformed_df_list = list()
  for(i in 1:length(transformations_list)){
    temp_df = data.frame( lapply(df[,!colnames(df) %in% dependent_var],
                                 transformations_list[[i]] ) )
    colnames(temp_df) = paste(  names(transformations_list)[i],
                                colnames(df[,!colnames(df) %in% dependent_var])  )
    transformed_df_list[[i]] = temp_df
  }
  
  df = dplyr::bind_cols(df,transformed_df_list)
  #head(df,5)
  
  # Ditching nonsense values
  ditch <- function(x) { ifelse(is.infinite(x) | is.nan(x), NA, x) }
  df =  data.frame(lapply(df,ditch))

  # Creating a correlation matrix with dependent variable
  cor_mat = cor(x = df,y = df[,dependent_var],use = 'pairwise.complete.obs')
  cor_df = setNames(data.frame(cor_mat),'correlation')
  # Calculating R2 value of dependent variable with MEFS
  cor_df$cor2 = (cor_df$correlation)^2
  # Filtering out low r2 values 
  low_cor2_value = quantile(cor_df$cor2,minimum_cor2_percentile)
  cor_df = cor_df[cor_df$cor2 >= low_cor2_value,]
  
  # Subsetting_df
  df = subset(df, select = rownames(cor_df))
  print('The variable choosen after r2 filtering are')
  print(colnames(df))
  
  #Findig out How many models to test
  print( paste0( 'No of linear regressions to Test are ',
                 choose(dim(df[,!colnames(df) %in% dependent_var])[2],no_of_factor) ) )
  
  # Making combinations of all possible Xs from df after removing y
  combinations = data.frame(combn(x = colnames(df[,!colnames(df) %in% dependent_var]),
                                  no_of_factor))
  
  # tc = t(combinations)
  
  combinations_list = list()
  
  for(i in 1:dim(combinations)[2]){
    # combinations_list[[i]] = list('Vars' = combinations[,i] )
    combinations_list[[i]] = as.character(combinations[,i])
  }
  
  
  # combinations_df = dplyr::bind_rows(combinations_list)
  combinations_df = data.frame('Vars' = I(combinations_list))
  
  # str(combinations_df)
  
  #Adding Dummys back to df
  df = cbind(df,dummy_df)
  
  # Colname Function
  col_names_fun = function(no_of_factor,dummy_vec){
    s = rep(NA,no_of_factor)
    s[1] = 'Intercept'
    for(i in 2:(1+no_of_factor)){
      s[i] = paste0('V',i-1)
    }
    return(c(s,dummy_vec))
  }
  
  
 
  col_names = col_names_fun(no_of_factor,dummy_vec)
  
  model_regression_func = function(varlist){
    # varlist = combinations_df$Vars[1]
    var_list = unlist(varlist)
    # var_list
    
    model_df = subset(df, select= c(dependent_var,var_list,dummy_vec))
    
    model = lm(as.formula(paste0(dependent_var,'~ .')), 
               data = model_df)
    
    #summary_model
    summary_model = summary(model)
    # summary_model
    
    adjr2 = summary_model$adj.r.squared
    
    if(adjr2 <= minimum_adjr2){
      return(NULL)
    }
    
    #Generating Coeff_df
    coeff_df =as.data.frame(t(coef(model)))
    
    #Singularity check function on p
    p_value_fun = function(var){
        return(ifelse(is.na(coeff_df[1,var]),
                            NA,
                            summary_model$coefficients[,4][var])
                      )
    }
    pvalues_df= as.data.frame(t( sapply(colnames(coeff_df),p_value_fun) ))
  
    # Replacing There Names
    coeff_df = setNames(coeff_df,paste0('coeff_',col_names))
    pvalues_df = setNames(pvalues_df,paste0('pvalue_',col_names))

    
    dof = summary_model$df[2]
    var_list_df = as.data.frame(matrix(data = var_list,nrow = 1))

    return(data.frame(var_list_df,coeff_df,pvalues_df,'adjr2' = adjr2,'dof' = dof))
  }
   
   
  if(parallel_computing){
      cl = makeCluster(detectCores())
      models_df = dplyr::bind_rows(parLapply(cl,combinations_df$Vars,model_regression_func))
      stopCluster(cl)
  } else{
      models_df = dplyr::bind_rows(lapply(combinations_df$Vars,model_regression_func))
  }
  
  #model_regression_func(combinations_df$Vars[[1]])
  
  if(nrow(models_df)== 0){
    print('The no regression combination has adjusted r2 higher then the specified')
    return()
  }
  # combinations_df$Vars[1]
  return(models_df[order(-models_df$adjr2),])
  
}


performance_bench = function(n,
                             df = gen_bench_df(), 
                             percentile = 0.5,
                             parallel_computing = TRUE,
                             y_var = 'street_fees',
                             dummy_vec = c('D1','D2'),
                             irrelevent_variables = c('iTraxx_5yr_senior'),
                             minimum_adjr2 = 0.7){
  start_time <- Sys.time()
  bench_df = gen_bench_df()
  colnames(bench_df)
  c = benchmarking(df = df, 
                   minimum_cor2_percentile = percentile,
                   minimum_adjr2 = minimum_adjr2,
                   parallel_computing = parallel_computing,
                    dependent_var = y_var,
                    dummy_vec = dummy_vec,
                   no_of_factor = n,
                  irrelevent_variables = irrelevent_variables)
  end_time <- Sys.time()
  print(end_time - start_time)
  print(head(c,10))
  return(c)
}






```



```{r benchmark.run}

dependent_var = 'street_fees'
mrm_df[,!colnames(mrm_df) %in% dependent_var]
head(mrm_df)

benchmark_run = function(percentile){
  
  folder_path = 'M:\\Model Validation\\IBCM EMEA DCM Model Validation\\R Code\\benchmarks\\'
  #One Factor
  print('One_factor Model')
  one_factor_models = performance_bench(1,percentile)
  saveRDS(one_factor_models,file = paste0(folder_path,'one_factor_models.rds'))
  write.csv(one_factor_models,paste0(folder_path,'one_factor_models.csv'))
  
  # Two Factor
  print('Two_factor Model')
  two_factor_models = performance_bench(2,percentile)
  saveRDS(two_factor_models,file = paste0(folder_path,'two_factor_models.rds'))
  write.csv(two_factor_models,paste0(folder_path,'two_factor_models.csv'))
  
  # Three Factor
  print('Three_factor Model')
  three_factor_models = performance_bench(3,percentile)
  saveRDS(three_factor_models,file = paste0(folder_path,'three_factor_models.rds'))
  write.csv(head(three_factor_models,50000),paste0(folder_path,'three_factor_models.csv'))
  
  return(list('one_factor_models' = one_factor_models,
              'two_factor_models'= two_factor_models,
              'three_factor_models' = three_factor_models))
}


models_list = benchmark_run(1,percentile = 0.0)

one_factor_models = performance_bench(2,0.5)

benchmarking(df = gen_bench_df(),no_of_factor = 3,dependent_var = 'street_fees',
             minimum_cor2_percentile = 0.95,parallel_computing = TRUE,minimum_adjr2 = 0.7)



```


```{r}

best_MRM_models = performance_bench(n = 3,
                                    percentile = 0,
                                    parallel_computing = TRUE,
                                    minimum_adjr2 = 0.74,irrelevent_variables = NULL)

tail(gen_bench_df())
```



```{r Benchmarking_selected.variables}


# Extracctign Variable for best 3 factor model
selected_vars = as.data.frame(read_xlsx('Variables to Test.xlsx', sheet = 'selected_variable'))
str(selected_vars)




three_factor_models_best_subset = performance_bench(3,percentile = 0,
                  irrelevent_variables =selected_vars$`Variable Name`[is.na(selected_vars$Include)] )

folder_path = 'M:\\Model Validation\\IBCM EMEA DCM Model Validation\\R Code\\benchmarks\\'
saveRDS(three_factor_models_best_subset,file = paste0(folder_path,'three_factor_models_best_subset.rds'))


write.csv(head(three_factor_models_best_subset,1000),paste0(folder_path,'three_factor_models_best_subset.csv'))


p_value_filter = function(df, p_alpha){
  fil_in = df$pvalue_Intercept <= p_alpha
  fil_c1 = df$pvalue_V1 <= p_alpha
  fil_c2 = df$pvalue_V2 <= p_alpha
  fil_c3 = df$pvalue_V3 <= p_alpha
  fil_d1 = df$pvalue_D1 <= p_alpha
  fil_d2 = df$pvalue_D2 <= p_alpha
  fil = fil_in & fil_c1 & fil_c2 & fil_c3 & fil_d1 & fil_d2
  return(df[fil,])
}

write.csv(head(p_value_filter(three_factor_models_best_subset,0.05),100),paste0(folder_path,'three_factor_models_best_100.csv'))


raptre_df = subset(gen_bench_df(), select = c('street'selected_vars$`Variable Name`[!is.na(selected_vars$Include)])



colnames(gen_bench_df())
selected_vars$`Variable Name`[!is.na(selected_vars$Include)]
```


```{r}

# Running model using sophie variables

choose(250,3)

md_vars = as.data.frame(read_xlsx('Variables to Test.xlsx',sheet = 'MD_Vars'))

md_vars = md_vars$Variabe
md_vars

all_vars = colnames(gen_bench_df())
all_vars


ignore_vars = all_vars[!all_vars %in% c(md_vars,'street_fees','D1','D2')]
ignore_vars

md_models = performance_bench(3,percentile = 0.8,minimum_adjr2 = 0.54,
                  irrelevent_variables = ignore_vars,parallel_computing = FALSE)

dim(md_models)
md_models_2 = performance_bench(3,percentile = 0.8,minimum_adjr2 = 0.7,
                  irrelevent_variables = ignore_vars,parallel_computing = TRUE)
dim(md_models_2)


# Segmenation Analysis 
md_models_FIG = performance_bench(n = 3,
                                df = gen_seg_df('FIG'),
                                percentile = 0.1,minimum_adjr2 = 0.7,
                                irrelevent_variables = ignore_vars,
                                parallel_computing = TRUE)

dim(md_models_CORP)

md_models_CORP = performance_bench(n = 3,
                                df = gen_seg_df('CORP'),
                                percentile = 0.1,minimum_adjr2 = 0.7,
                                irrelevent_variables = ignore_vars,
                                parallel_computing = F)

# 
write.csv(md_models_CORP,paste0(folder_path,'best subset models_street_fees_CORP.csv'))


# 
# read.csv(paste0(folder_path,'best subset models with iTraxx senior.csv'))
# 
# models_df_iTraxx = read.csv('benchmarks/best subset models_with iTraxx senior.csv')
# 
# write.csv(head(p_value_filter(models_df_iTraxx,0.05),200),paste0(folder_path,'best subset models_with iTraxx senior.csv'))

```



```{r}
# Checking for iTraxx vairalbe 

itraxx_qtr_avg = read.csv('itraxx_qtravg.csv')

mrm_df_itraxx_qtr_avg = cbind(mrm_df, itraxx_qtr_avg)
colnames(mrm_df_itraxx_qtr_avg)
summary(lm(street_fees ~   QoQ_CDX+ itraxx_qtr_avg + eur_gdp + D1 + D2 , data = mrm_df_itraxx_qtr_avg))

summary(lm_mrm)

```



# SOW Analysis 
```{r SoW}

# Gettign the Data of SoW

gen_sow_df = function(start_t = '2006 Q1'){
  start_t = as.yearqtr(start_t,format = "%Y Q%q ")
  sow_df = as.data.frame(read_xlsx('Data and Calculation_1.xlsx',sheet = 'sow'))

  # Creating yearqtr Variable
  sow_df$time = as.yearqtr(sow_df$Quarter,format = "Q%q %Y")
  
  sow_df = sow_df[sow_df$time >=start_t,]

  # Removing Quarter
  sow_df$Quarter = NULL
  
  sow_df = setNames(sow_df,c('street_fees','cs_fees','time'))
  # Calculating Sow
  sow_df$sow = sow_df$cs_fees/sow_df$street_fees
  
  # Putting t
  sow_df$t = 1:nrow(sow_df)
  
  return(sow_df[,c(3,5,1,2,4)])
}



sow_cal = function(no.of.qtr.to.avg, 
                   no.of.qtr.taken.to.avg ,
                   df,
                   jop_t = '2017 Q3',
                   projection_horizon = 8){
  
  jop_t = as.yearqtr(jop_t,format = "%Y Q%q")
  index = which(jop_t == df$time)
  
  jop = df$cs_fees[index]/df$street_fees[index]
  
  avg_range = (index - no.of.qtr.to.avg+1) :(index) 
  
  avg = sum(df$cs_fees[avg_range])/sum(df$street_fees[avg_range])    
  #print(paste0('Jump of point = ',jop))
  #print(paste0('Long Term Average = ',avg))
  v = sapply(1:projection_horizon,function(i){
            ifelse( i < no.of.qtr.taken.to.avg
                   , ( (no.of.qtr.taken.to.avg-i)/no.of.qtr.taken.to.avg ) * jop + (i/no.of.qtr.taken.to.avg)*avg        
                   ,avg)})
  return(v)
}


projection_analysis_sow = function(start_Q,
                                  end_Q,
                                  df,
                                  no.of.qtr.to.avg = 12, 
                                  no.of.qtr.taken.to.avg = 4,
                                  model = lm_mrm){
  
  # Filtering
  n = nrow(df)
  start_t = df[df$time == start_Q,'t']
  end_t = df[df$time == end_Q,'t']
  

  
  outsample = (df$t >= start_t &  df$t <= end_t)
  insample = !outsample
  
  
  # Model Calibration on Insample Data
  
  # print("Model Peformance")
  # print(summary(model))
  
  # Model Prediction using Outsample Data
  df$predicted.street.fees = predict(object = model, newdata = df)
  
  
  # Calculationg SoW 
  sow = sow_cal(no.of.qtr.to.avg,
                no.of.qtr.taken.to.avg,
                df,
                jop_t = df$time[start_t -1],
                projection_horizon = (end_t - start_t + 1) ) 

  df$predicted.sow = c( rep(NA,start_t-1),
                        sow,
                        rep(NA, n - end_t) )
  # Caculation of actual Cs Fees
  df$predicted.cs_fees = df$predicted.street.fees * df$predicted.sow
  
  #RMSE Print
  # print('Model Performance as follows')
  performance_df = performance_metric(df,
                     outsample, 
                     is_mar_and_sd_out_of_filter = T,
                     select_vars = c('cs_fees','predicted.cs_fees'))
  # print(performance_df)
  
  
  # Calculation for checking the sum
  # List Output
  actual = sum(df[outsample,]$cs_fees)
  predicted = sum(df[outsample,]$predicted.cs_fees)
  
  output = vector(mode = 'list',length = 3)
  names(output) = c('predicted','actual','actual_band')
  
  output$actual = actual
  output$predicted = predicted
  output$actual_band = actual*1.1 # For 10% buffer on actual
  
  # print(output)
  
  # Savings Values to peformance_df
  performance_df = rbind(performance_df,
                         "actual_street_fees_total" = sum(df[outsample,]$street_fees),
                         "predicted_street_fees_total" = sum(df[outsample,]$predicted.street.fees),
                        "actual_cs_fees_total" = actual,
                        "predicted_cs_fees_total" = predicted)
  
  df$actual_street_fees = df$street_fees
  df$actual_cs_fees = df$cs_fees
  df$actual_SoW = df$SoW
  df$cs_fees = NULL
  df$SoW = NULL
  df$street_fees = NULL
  
  cs_fees_plot = draw_time_series_plot(df = subset(df[outsample,],select = c('time','actual_cs_fees','predicted.cs_fees'))
                        ,y_label = 'Model performance'
                        ,x_label = 'Time', legend = 'right')
  street_fees_plot = draw_time_series_plot(df = subset(df[outsample,],select = c('time','actual_street_fees','predicted.street.fees'))
                        ,y_label = 'Street Fees Projection'
                        ,x_label = 'Time', legend = 'bottom')
  sow_plot = draw_time_series_plot(df = subset(df[outsample,],select = c('time','actual_SoW','predicted.sow'))
                        ,y_label = 'SoW Projection'
                        ,x_label = 'Time', legend = 'bottom')
  grob_to_plot = arrangeGrob(cs_fees_plot,
                             street_fees_plot,
                             sow_plot,
                             widths = c(1, 1),
                             layout_matrix = matrix(c(1,1,2,3), nrow=2, byrow=TRUE))
  

  
  folder_path = "M:\\Model Validation\\IBCM EMEA DCM Model Validation\\R Code\\Report Pictures\\sow\\"
  file_path = paste0("CS Street Fees in performance from ",start_Q,' to ',end_Q,' with ',
                     ' no.of.qtr.to.avg ',no.of.qtr.to.avg,
                     ' no.of.qtr.taken.to.avg ',no.of.qtr.taken.to.avg)
  
  # # Saving the Graphs in the folder 
  plot_path = paste0(folder_path,file_path,".png")
  ggsave(plot_path, plot = grob_to_plot,dpi = 300,width = 12,height = 10)

  return(performance_df)
}


```

```{r}

rbind(p_df,"a" = 2)

```



```{r}
extended_mrm_df = create_basic_data(input_file_path = 'emea_dcm_mrm_testing_data_latest_data.csv')

p_df = projection_analysis_sow('2015 Q4','2017 Q3',df = extended_mrm_df,
                                no.of.qtr.to.avg =20,no.of.qtr.taken.to.avg = 0)

rownames(p_df)

jump_of_list = list('jop_2015_Q3' = c('2015 Q4','2017 Q3'),
                'jop_2015_Q4' = c('2016 Q1','2017 Q4'),
                'jop_2016_Q1' = c('2016 Q2','2018 Q1'),
                'jop_2016_Q2' = c('2016 Q3','2018 Q2'),
                'jop_2016_Q3' = c('2016 Q4','2018 Q2'),
                'jop_2016_Q4' = c('2017 Q1','2018 Q2'))

#X = no.of.qtr.to.avg , Y = no.of.qtr.taken.to.avg
XY = list('X = 8,Y = 4' = c(8,4),
          'X = 12,Y = 4' = c(12,4),
          'X = 16,Y = 4' = c(16,4),
          'X = 20,Y = 4' = c(20,4),
          'X = 8,Y = 0' = c(8,0),
          'X = 12,Y = 0' = c(12,0),
               'X = 16,Y = 0' = c(16,0),
               'X = 20,Y = 0' = c(20,0))

names(jop_list)[2]

sow_compare = function(jop_list = jump_of_list,XY_list = XY){
  for(i in 1:length(jop_list)){
    print(paste('Calcualtions for',names(jop_list)[i]))
    p_df_list = list()
    for(j in 1:length(XY_list)){
      print(paste('Calcualtions for',names(XY_list)[j]))
      p_df_list[[j]] = setNames(projection_analysis_sow(jop_list[[i]][1],
                                               jop_list[[i]][2],
                                               df = extended_mrm_df,
                                               no.of.qtr.to.avg = XY_list[[j]][1],
                                               no.of.qtr.taken.to.avg = XY_list[[j]][2]),
                                names(XY_list)[j])
      
    }
    p_df = as.data.frame(dplyr::bind_cols(unname(p_df_list)))
    rownames(p_df) = c('RMSE','MAE','MAR','SD',
                       'CERRPCT','RMSE/SD','MAE/MAR',
                       'actual_street_fees_total',
                       'predicted_street_fees_total',
                       'actual_cs_fees_total',
                       'predcited_cs_fees_total')
    
    folder_path = "M:\\Model Validation\\IBCM EMEA DCM Model Validation\\R Code\\Report Pictures\\sow\\"
    write.csv(x = p_df,file = paste0(folder_path,
                                     'cs_fees peformance for jop = ',
                                     names(jop_list)[i],'.csv'))
  }
}


sow_compare(jop_list = jump_of_list,XY_list = XY)


draw_time_series_plot(subset(extended_mrm_df,select = c('time','SoW')),
                      y_label = 'SoW',text_angle = 90)
```

```{r}

```



```{r}




projection.period = function(jop_t,df,
                             period.default = 8){
  index = which(jop_t == df$time)
  n = nrow(df)
  return( ifelse((index + period.default) > n, n-index, period.default) )
}

RMSE_sow = function(m,n,df = mrm_df ,
                    period.default = 8){
  jop_vec = df$time[m:(nrow(df)-1)]
  jop_list = lapply(jop_vec, function(jop_t){
    index = which(jop_t == df$time)
    
    t = projection.period(jop_t,df,period.default)
    sow = sow_cal(no.of.qtr.to.avg = m,
                  no.of.qtr.taken.to.avg = n,
                  df = df,
                  jop_t = jop_t,
                  projection_horizon = t)
    return(cbind(subset(df[(index+1):(index+t),],
                        select = c('time','street_fees','cs_fees','SoW')), 
                 data.frame('Jump of Point' = jop_t,
                              'predicted.sow' = sow)
                 )
           )
    })
  jop_df = dplyr::bind_rows(jop_list)
  rmse = RMSE(jop_df$predicted.sow,jop_df$SoW)
  #mae = MAE(jop_df$predicted.sow,jop_df$SoW)  
  return(rmse)
}

optimal_sow = function(mean.number.of.qtrs,
                       time.number.of.qtrs,
                       df = mrm_df,
                       projection.horizon = 8){
    
    start_t = Sys.time()
    choice_df_list = lapply(mean.number.of.qtrs,function(m){
                        return(data.frame('No of Qtrs used for avg' = m, 
                                          'No of Qtrs taken to reach avg' = time.number.of.qtrs))})
                              
    choice_df = dplyr::bind_rows(choice_df_list)
    
    choice_df$RMSE = mapply(RMSE_sow,choice_df$No.of.Qtrs.used.for.avg,
                            choice_df$No.of.Qtrs.taken.to.reach.avg,MoreArgs = list('period.default' = projection.horizon,
                                                                                    'df' = df))
    end_t = Sys.time()
    print(paste('Time taken to complete is ',end_t - start_t))
    return(choice_df[order(choice_df$RMSE),])
}


```

```{r}

optimal_choice_sow_df = optimal_sow(1:50,0:50,df = mrm_df)


head(optimal_choice_sow_df,100)


write.csv(optimal_choice_sow_df,paste0(folder_path,'optimal_choce_sow_with_20qtrs_as_max.csv'))

optimal_choice_sow_df$rank = 1:nrow(optimal_choice_sow_df)

rank = optimal_choice_sow_df[optimal_choice_sow_df$No.of.Qtrs.used.for.avg == 12 & optimal_choice_sow_df$No.of.Qtrs.taken.to.reach.avg == 4,'rank']
optimal_choice_sow_df[rank,]

str(optimal_choice_sow_df)


RMSE_sow(1,4,df = mrm_df)
RMSE_sow(12,8,df = mrm_df)
tail(sow_df)
sow_cal(2,0,df = mrm_df)


RMSE_sow(20,2)



```


```{r}

gen_sow_M1_df = function(sow,m){
  M1_list = lapply( (m+1):length(sow),
                   function(n){
                     return( data.frame( 'MS1' = sow[n],'JOP' = sow[n-1],'AVG' = mean( sow[(n-m):(n-1)])))
                   })
  
  return(dplyr::bind_rows(M1_list))
}




gen_sow_M2_df = function(sow,m){
  M2_list = lapply( (m+2):length(sow),
                   function(n){
                     return( data.frame( 'MS2' = sow[n],'JOP' = sow[n-2],'AVG' = mean( sow[(n-m-1):(n-2)])))
                   })
  
  return(dplyr::bind_rows(M2_list))
}



# M3 dataframe
gen_sow_M3_df = function(sow,m){
  M3_list = lapply( (m+3):length(sow),
                   function(n){
                     return( data.frame( 'MS3' = sow[n],'JOP' = sow[n-3],'AVG' = mean( sow[(n-m-2):(n-3)])))
                   })
  
  return(dplyr::bind_rows(M3_list))
}



```

```{r}



sow_df = gen_sow_df()
head(sow_df,13)
tail(sow_df)
M1_df = gen_sow_M1_df(sow_df$sow,12)

head(M1_df)
tail(M1_df)
summary(lm(MS1 ~ JOP + AVG -1 ,data = M1_df))
summary(lm(MS1 ~ JOP + AVG  ,data = M1_df))

M2_df = gen_sow_M2_df(sow_df$sow,12)
summary(lm(MS2 ~ JOP + AVG -1,data = M2_df))
summary(lm(MS2 ~ JOP + AVG ,data = M2_df))

M3_df = gen_sow_M3_df(sow_df$sow,12)
summary(lm(MS3 ~ JOP + AVG -1,data = M3_df))
summary(lm(MS3 ~ JOP + AVG ,data = M3_df))


```



```{r}
# mega_df = data_input_output('2017 Q4')
# mega_df[['baseline_scenario']]
# mega_df[['euro_scenario']]
# mega_df[['china_scenario']]
# 
# names(mega_df)

cs_fees_surface_point = function(no.of.qtr.to.avg = 12,
                       no.of.qtr.taken.to.avg = 4 ,
                       df = mrm_df,scenario = 'baseline_scenario'){
          jop_t = '2017 Q3'
          index = which(jop_t == df$time)
          sow = sow_cal(no.of.qtr.to.avg,
                        no.of.qtr.taken.to.avg,
                        df,
                        jop_t)
          street_fees = predict(object = lm_mrm,data_input_output('2017 Q4')[[scenario]])
          cs_fees = street_fees*sow
          return( sum(cs_fees) )
}


cs_fees_surface = function(mean.number.of.qtrs,
                       time.number.of.qtrs,
                       df = mrm_df,
                       phi = 40,
                       theta = 60,
                       scenario = 'baseline_scenario'){
    
    choice_df_list = lapply(mean.number.of.qtrs,function(m){
                        return(data.frame('No of Qtrs used for avg' = m, 
                                          'No of Qtrs taken to reach avg' = time.number.of.qtrs))})
                              
    choice_df = dplyr::bind_rows(choice_df_list)

    choice_df$cs_fees = mapply(cs_fees_surface_point,
                               choice_df[,1],
                               choice_df[,2],MoreArgs = list('df' = df,'scenario' = scenario))
    scatter3D(x = choice_df[,1],y = choice_df[,2],z = choice_df[,3],
              cex = 0.5, pch = 1,phi = phi, theta = theta,
              ticktype = 'detailed',bty = 'g',
              xlab ='Historical Qtrs Avg',
              ylab = 'Qtrs Duration',
              zlab = 'CS fees',main = scenario)
    return(choice_df)
}







```


```{r}

cs_fees_baseline_df = cs_fees_surface(1:20,0:20)
choice_df = cs_fees_baseline_df
  scatter3D(x = choice_df[,1],y = choice_df[,2],z = choice_df[,3],
              cex = 0.5, pch = 1,phi = 90, theta = 0,
              ticktype = 'detailed',bty = 'g',
              xlab ='Historical Qtrs Avg',
              ylab = 'Qtrs Duration',
              zlab = 'CS fees',main = 'scenario')

cs_fees_euro_df = cs_fees_surface(1:20,0:20,scenario = 'euro_scenario')
cs_fees_china_df = cs_fees_surface(1:20,0:20,scenario = 'china_scenario')

write_xlsx(cs_fees_baseline_df[cs_fees_baseline_df$No.of.Qtrs.taken.to.reach.avg ==4,], 'time is fixed.xlsx')
write_xlsx(cs_fees_baseline_df[cs_fees_baseline_df$No.of.Qtrs.used.for.avg ==12,], 'avg is fixed.xlsx')

cs_fees_euro_df[cs_fees_euro_df$No.of.Qtrs.used.for.avg ==12,]
cs_fees_china_df[cs_fees_china_df$No.of.Qtrs.used.for.avg ==12,]


```



```{r Time Series Modeling on SoW}

mean(sow_df$sow[1:12])


draw_time_series_plot(sow_df[,c(1,5)],'sow')

stationary_check(mef_list = 'sow',relevent_data = sow_df)



# ACF & PACF plots of sow
acf(sow_df$sow, main = 'ACF of Residuals',lag.max = 20)
pacf(sow_df$sow,main = 'PACF of Residuals',lag.max = 20)
pacf_band = 1/sqrt(46)
pacf_band


  
#Making a Plots  
head(sow_df)
sow_df$quarter = format(x = sow_df$time,format = 'Q%q')
sow_df$quarter = as.factor(sow_df$quarter)


# Box Plot
boxplot(sow ~ quarter, data = sow_df)
  
# Kruskal Wallis Test # For all quearters together
kruskal.test(sow ~ quarter, data = sow_df)

# Regressing

sow_df

sow_lm = lm(sow_df$sow ~ Lag(sow_df$sow,shift = 3))
sow_lm = lm(sow ~ lag(sow,n = 3),data = sow_df)
summary(sow_lm)
colnames(sow_df)


draw_time_series_plot(sow_df[,c(1,7)],'lag.1.sow')


sow_df$lag.1.sow = lag(sow_df$sow,1)
sow_df$L1.diff = sow_df$sow - sow_df$lag.1.sow
sow_df$L2.diff = sow_df$sow - lag(sow_df$sow,n = 2)
sow_df$QoQ_sow = qoq_p(sow_df$sow)


draw_time_series_plot(sow_df[,c(1,9)],'lag.1.sow')
stationary_check(mef_list = 'L1.diff',relevent_data = sow_df[-1,])


sow_lm = lm((sow-lag(sow,1))~ lag(sow,1) + lag(L2.diff,1) ,data = sow_df)
summary(sow_lm)

summary(lm(sow ~ 1, data = sow_df))


# ACF & PACF plots of sow
acf(sow_df$L1.diff[-1], main = 'ACF of Residuals',lag.max = 20)
pacf(sow_df$L1.diff[-1],main = 'PACF of Residuals',lag.max = 20)

# Building AR2 Model
sow_df$sow.mean = sow_df$sow - mean(sow_df$sow)
sow_df$lag.1.sow.mean = sow_df$lag.1.sow - mean(sow_df$sow)
sow_df$lag.2.sow.mean = lag(sow_df$sow,2) - mean(sow_df$sow)
sow_df$lag.3.sow.mean = lag(sow_df$sow,3) - mean(sow_df$sow)

head(sow_df)

sow_lm_2 = lm(sow.mean ~ lag.1.sow.mean+ lag.2.sow.mean+ lag.3.sow.mean,data = sow_df)
summary(sow_lm_2)


summary(lm(sow ~ lag(sow,1)+ lag(sow,2,data = sow_df)))


```


```{r}
# Bootstrapping Things
# length(sample(1:47,30,replace = TRUE))
# 
# bootstrap_input_func(mrm_df,sample(1:47,30,replace = TRUE))

bootstrap_input_func = function(data,index){
  return(coef(lm(street_fees ~   QoQ_CDX+ iTraxx + eur_gdp + D1 + D2, data = data, subset = index)))
}

do.bootstrap = function(df = mrm_df,simulations = 100){
  start_t = Sys.time()
  boot_reg = boot(df, bootstrap_input_func, simulations)
  end_t = Sys.time()
  print(paste0("time taken for executions is ",end_t - start_t))
  return(boot_reg)
}

dcm.boot = do.bootstrap(simulations = 1e4)


# Comparing the results of bootstrapping and model
dcm.boot.matrix = as.matrix(dcm.boot)
dcm.boot.matrix

dcm.boot$t0
dcm.boot


summary(lm_mrm)
dcm.boot$statistic()

# See the contents in boot stap regressiongs
dcm.boot.df = as.data.frame(boot.array(dcm.boot))
head(dcm.boot.df)

#QoQ_CDX
plot(dcm.boot, index = 2)

#iTraxx
plot(dcm.boot, index = 3)

#Euro GDP
plot(dcm.boot, index = 4)

#D1
plot(dcm.boot, index = 5)

#D2
plot(dcm.boot, index = 6)

dcm.coeff_df = as.data.frame(dcm.boot$t)
head(dcm.coeff_df)




# Plotting Confidence interval
a =boot.ci(dcm.boot,index = 2,type = c('norm','perc','bca'))

str(data.frame(a$percent,row.names = 'QoQ_CDX'))

data.frame(a$percent,row.names = 'QoQ_CDX')

#Jacknife after bootstrap
par(mfcol = c(2,1))
jack.after.boot(dcm.boot, index = 2, main = 'QoQ Coefficient')
jack.after.boot(dcm.boot, index = 3, main = 'iTraxx')

names(coef(lm_mrm))

```


```{r}


```

# Model Output Analysis
```{r}
model_output = function(cut_off_quarter = '2017 Q4',
                        model = lm_mrm,
                        df = mrm_df,
                        angle = 45){
  #Predictions in df
  df$predicted.street_fees = predict(model,df)
  
  mega_df = data_input_output(cut_off_quarter)
  baseline_df = mega_df$baseline_scenario
  euro_df = mega_df$euro_scenario
  china_df = mega_df$china_scenario
  
  baseline.street_fees = predict(model,baseline_df) 
  LPAEuro.street_fees = predict(model, euro_df)
  LPAChina.street_fees = predict(model, china_df)
  
  projection_df = data.frame('sow' = sow_cal(no.of.qtr.to.avg = 12,4,df = df,jop_t = '2017 Q3'),
                              'Baseline' = baseline.street_fees,
                             'LPA_Euro' = LPAEuro.street_fees,
                             'LPA_China' = LPAChina.street_fees)
  
  projection_df$Baseline.cs_fees = projection_df$Baseline * projection_df$sow
  projection_df$LPA_Euro.cs_fees = projection_df$LPA_Euro * projection_df$sow
  projection_df$LPA_China.cs_fees = projection_df$LPA_China * projection_df$sow
  
  folder_path = "M:\\Model Validation\\IBCM EMEA DCM Model Validation\\R Code\\Report Pictures\\model_output\\"  
  
  # Saving Result of Projection
  res = as.data.frame(t(projection_df))
  # write.xlsx(res,path = paste0(folder_path,'projections.xlsx'),row.names = T)
  write.csv(res,file = paste0(folder_path,'projections.csv'))
  
  df = merge(x = df,y = projection_df,by = 'row.names',all = T)
  df$time = as.character(df$Row.names)
  df$Row.names = NULL
  
  # Printing Graphs
  
  pl = draw_time_series_plot(subset(df,
                                    select = c('time',
                                               'street_fees',
                                               'predicted.street_fees',
                                               'Baseline',
                                               'LPA_Euro',
                                               'LPA_China')),
                             y_label = 'Street Fees',
                             x_label = 'Time', legend = 'top', text_angle = angle)
  ggsave(plot = last_plot(),filename = paste0(folder_path,'street_fees_plot.png'),
         dpi = 300,
         width = 12,height = 6)
  
  # Ploting Scenarios street fees from Projection df
  pl = draw_time_series_plot(subset(data.frame(projection_df,
                                               'time' = rownames(projection_df)),
                                    select = c('time',
                                               'Baseline',
                                               'LPA_Euro',
                                               'LPA_China')),
                             y_label = 'Street Fees',
                             x_label = 'Time', legend = 'top', text_angle = angle)
  
  ggsave(plot = last_plot(),filename = paste0(folder_path,'scenario_fees_plot.png'),
         dpi = 300,
         width = 12,height = 6)
  
  # Plotting SoW
  pl = draw_time_series_plot(subset(data.frame(projection_df,
                                               'time' = rownames(projection_df)),
                                    select = c('time','sow')),
                             y_label = 'Share of Wallet (Market Share)',
                             x_label = 'Time', legend = 'top')
  
  ggsave(plot = last_plot(),filename = paste0(folder_path,'sow_plot.png'),
         dpi = 300,
         width = 12,height = 6)
  
  # Plotting CS Fees
  pl = draw_time_series_plot(subset(df,
                                    select = c('time',
                                               'cs_fees',
                                               'Baseline.cs_fees',
                                               'LPA_Euro.cs_fees',
                                               'LPA_China.cs_fees')),
                             y_label = 'CS Fees',
                             x_label = 'Time', legend = 'top', text_angle = angle)
  ggsave(plot = last_plot(),filename = paste0(folder_path,'cs_fees_all_time_plot.png'),
         dpi = 300,
         width = 12,height = 6)
  
  # Ploting CS fees in different scenarios from Projection df
  pl = draw_time_series_plot(subset(data.frame(projection_df,
                                               'time' = rownames(projection_df)),
                                    select = c('time',
                                               'Baseline.cs_fees',
                                               'LPA_Euro.cs_fees',
                                               'LPA_China.cs_fees')),
                             y_label = 'CS Fees',
                             x_label = 'Time', legend = 'top')
  
  ggsave(plot = last_plot(),filename = paste0(folder_path,'cs_fees_scenario_plot.png'),
         dpi = 300,
         width = 12,height = 6)
  
    
  return(df)
  
}

```



```{r}
df = model_output(df = extended_mrm_df,angle = 90)
tail(df,10)

str(df)
df$time = as.yearqtr(as.character(df$time),format = "%Y Q%q")

as.data.frame(t(output_df))

rownames(mrm_df)
rownames()



tail(df,10)
colnames(df)
```


```{r}



#  Plotting the results 
scenario_projection_analysis_2 = function(df = extended_mrm_df,
                                          start_t = "2016 Q4", 
                                          end_t = "2018 Q3" ,
                                        model = lm_mrm,
                                        cut_off_q = "2017 Q1"){
  
  start_t = "2006 Q1"
  
  # filtering relevent df values
  start_t = as.yearqtr(start_t, format = "%Y Q%q")
  end_t = as.yearqtr(end_t, format = "%Y Q%q")
  df$time = as.yearqtr(df$time,format = "%Y Q%q" )
  
  
  # Getting the Stress DFs
  mega_df= data_input_output(cut_off_quarter = cut_off_q)
  
  
  # Making Revenue Projection and Plotting
  temp_df = subset(df[df$time >= start_t & df$time <= end_t,], select = c('street_fees'))
  
  LPA_Euro_Street_fees  = setNames(as.data.frame(predict(model,mega_df$euro_scenario)),
                                   'LPA Euro Street Fees')
  
  LPA_China_Street_fees = setNames(as.data.frame(predict(model, mega_df$china_scenario)),
                                   'LPA China Street Fees')
  
  SFTQ_street_fees = setNames(as.data.frame(predict(model, mega_df$sftq_scenario)),
                              'SFTQ Street fees')
  
  EMHL_street_fees = setNames(as.data.frame(predict(model, mega_df$emhl_scenario)),
                              'EMHL Street fees')

  street_fees_list <- list( actual = temp_df,
                  euro= LPA_Euro_Street_fees, 
                  china= LPA_China_Street_fees, 
                  sftq= SFTQ_street_fees,
                  emhl = EMHL_street_fees)
  
  
  temp_df = as.data.frame(multimerge(street_fees_list))
  
  temp_df = setNames(temp_df,
                     c('Actual','LPA Euro','LPA China','SFTQ','EMHL'))
  temp_df$time = rownames(temp_df)
  
  
  
  revenue_plot = draw_time_series_plot(temp_df, y_label = 'Projected Revenue in different scenarios',
                                       legend = 'right',text_angle = 90)
  # Plotting MEFs as well
  var_list = c('CDX','QoQ_CDX','iTraxx','eur_gdp')
  for(mef in var_list){
      mef_list = list()
      mef = 'iTraxx'
      
      mef_list[[paste0('actual ',mef)]] = subset(df[df$time >= start_t & df$time <= end_t,], select = c(mef))
      mef_list[[paste0('LPA_Euro ',mef)]] = subset(mega_df[[2]],select = mef)
      mef_list[[paste0('LPA_China ',mef)]] = subset(mega_df[[3]],select = mef)
      mef_list[[paste0('SFTQ ',mef)]] = subset(mega_df[[4]],select = mef)
      mef_list[[paste0('EMHL ',mef)]] = subset(mega_df[[5]],select = mef)
  
      temp_df = as.data.frame(multimerge(mef_list))
      temp_df$time = rownames(temp_df)
    
      plot = draw_time_series_plot(temp_df, y_label = paste( mef,'in different scenarios'),
                                    text_angle = 90, legend = 'top')
      assign(paste0(mef,'_plot'),plot)
  }
  
  
  print(plot)
  grob_to_plot = arrangeGrob(revenue_plot,CDX_plot,QoQ_CDX_plot,iTraxx_plot,eur_gdp_plot,
              widths = c(1, 1),
              layout_matrix = matrix(c(1,1,2,3,4,5), nrow=3, byrow=TRUE)
              )
  

  
  folder_path = "M:\\Model Validation\\IBCM EMEA DCM Model Validation\\R Code\\Report Pictures\\scenario\\"
  file_path = paste0("Scenario Projections  from ",start_t,' to ',end_t)
  
  # # Saving the Graphs in the folder 
  plot_path = paste0(folder_path,file_path,".png")
  ggsave(plot_path, plot = grob_to_plot,dpi = 300,width = 15,height = 10)        


  
  #Plotting the MEFs as well
  #return(revenue_plot)
}

```


```{r}
scenario_projection_analysis_2()
scenario_projection_analysis_2(start_t = "2007 Q1")
```

